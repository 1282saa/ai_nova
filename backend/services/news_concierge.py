"""
AI ë‰´ìŠ¤ ì»¨ì‹œì–´ì§€ ì„œë¹„ìŠ¤

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ì—¬ AIê°€ ìƒì„±í•œ ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ê³¼ ê°ì£¼ ì‹œìŠ¤í…œì„ ì§€ì›í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import re
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, AsyncGenerator, Tuple
import openai
from pydantic import BaseModel, Field
from collections import Counter

from backend.api.clients.bigkinds import BigKindsClient
from backend.utils.query_processor import QueryProcessor
from backend.utils.logger import setup_logger
from .news.related_questions_generator import RelatedQuestionsGenerator


class ConciergeRequest(BaseModel):
    """AI ë‰´ìŠ¤ ì»¨ì‹œì–´ì§€ ìš”ì²­ ëª¨ë¸"""
    question: str = Field(..., description="ì‚¬ìš©ì ì§ˆë¬¸", min_length=2, max_length=500)
    date_from: Optional[str] = Field(None, description="ê²€ìƒ‰ ì‹œì‘ì¼ (YYYY-MM-DD)")
    date_to: Optional[str] = Field(None, description="ê²€ìƒ‰ ì¢…ë£Œì¼ (YYYY-MM-DD)")
    max_articles: int = Field(default=10, description="ìµœëŒ€ ê²€ìƒ‰ ê¸°ì‚¬ ìˆ˜", ge=5, le=50)
    include_related_keywords: bool = Field(default=True, description="ì—°ê´€ì–´ í¬í•¨ ì—¬ë¶€")
    include_today_issues: bool = Field(default=True, description="ì˜¤ëŠ˜ì˜ ì´ìŠˆ í¬í•¨ ì—¬ë¶€")
    include_related_questions: bool = Field(default=True, description="ê´€ë ¨ ì§ˆë¬¸ í¬í•¨ ì—¬ë¶€")
    detail_level: str = Field(default="detailed", description="ë‹µë³€ ìƒì„¸ë„ (brief/detailed/comprehensive)")


class ArticleReference(BaseModel):
    """ê¸°ì‚¬ ì°¸ì¡° ì •ë³´"""
    ref_id: str = Field(description="ì°¸ì¡° ID (ref1, ref2 ë“±)")
    title: str = Field(description="ê¸°ì‚¬ ì œëª©")
    provider: str = Field(description="ì–¸ë¡ ì‚¬")
    published_at: str = Field(description="ë°œí–‰ì¼ì‹œ")
    url: Optional[str] = Field(None, description="ê¸°ì‚¬ URL")
    relevance_score: float = Field(description="ê´€ë ¨ë„ ì ìˆ˜", ge=0, le=1)


class ConciergeResponse(BaseModel):
    """AI ë‰´ìŠ¤ ì»¨ì‹œì–´ì§€ ì‘ë‹µ ëª¨ë¸"""
    question: str = Field(description="ì›ë³¸ ì§ˆë¬¸")
    answer: str = Field(description="AI ë‹µë³€ (ê°ì£¼ í¬í•¨)")
    summary: str = Field(description="í•µì‹¬ ìš”ì•½")
    key_points: List[str] = Field(description="ì£¼ìš” í¬ì¸íŠ¸")
    references: List[ArticleReference] = Field(description="ì°¸ì¡° ê¸°ì‚¬ ëª©ë¡")
    related_keywords: List[str] = Field(default=[], description="ì—°ê´€ í‚¤ì›Œë“œ")
    related_questions: List[Dict[str, Any]] = Field(default=[], description="ì—°ê´€ ì§ˆë¬¸")
    today_issues: List[Dict[str, Any]] = Field(default=[], description="ê´€ë ¨ ì˜¤ëŠ˜ì˜ ì´ìŠˆ")
    search_strategy: Dict[str, Any] = Field(description="ì‚¬ìš©ëœ ê²€ìƒ‰ ì „ëµ")
    analysis_metadata: Dict[str, Any] = Field(description="ë¶„ì„ ë©”íƒ€ë°ì´í„°")
    generated_at: str = Field(description="ìƒì„± ì‹œê°„")


class ConciergeProgress(BaseModel):
    """ì»¨ì‹œì–´ì§€ ì§„í–‰ ìƒí™©"""
    stage: str = Field(description="í˜„ì¬ ë‹¨ê³„")
    progress: int = Field(description="ì§„í–‰ë¥  (0-100)", ge=0, le=100)
    message: str = Field(description="ì§„í–‰ ë©”ì‹œì§€")
    current_task: Optional[str] = Field(None, description="í˜„ì¬ ì‘ì—…")
    extracted_keywords: Optional[List[str]] = Field(None, description="ì¶”ì¶œëœ í‚¤ì›Œë“œ")
    search_results_count: Optional[int] = Field(None, description="ê²€ìƒ‰ ê²°ê³¼ ìˆ˜")
    streaming_content: Optional[str] = Field(None, description="ìŠ¤íŠ¸ë¦¬ë° ì»¨í…ì¸ ")
    result: Optional[ConciergeResponse] = Field(None, description="ìµœì¢… ê²°ê³¼")


class NewsConciergeService:
    """AI ë‰´ìŠ¤ ì»¨ì‹œì–´ì§€ ì„œë¹„ìŠ¤"""
    
    def __init__(self, openai_api_key: str, bigkinds_client: BigKindsClient):
        """
        Args:
            openai_api_key: OpenAI API í‚¤
            bigkinds_client: BigKinds í´ë¼ì´ì–¸íŠ¸
        """
        self.openai_api_key = openai_api_key
        self.bigkinds_client = bigkinds_client
        self.query_processor = QueryProcessor()
        self.questions_generator = RelatedQuestionsGenerator()
        self.logger = setup_logger("services.news_concierge")
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        openai.api_key = self.openai_api_key
    
    async def generate_concierge_response_stream(
        self, 
        request: ConciergeRequest
    ) -> AsyncGenerator[ConciergeProgress, None]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ AI ë‰´ìŠ¤ ì»¨ì‹œì–´ì§€ ì‘ë‹µ ìƒì„±
        
        Args:
            request: ì»¨ì‹œì–´ì§€ ìš”ì²­
            
        Yields:
            ConciergeProgress: ì§„í–‰ ìƒí™©
        """
        start_time = time.time()
        
        try:
            # 1ë‹¨ê³„: ì§ˆë¬¸ ë¶„ì„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ
            yield ConciergeProgress(
                stage="question_analysis",
                progress=5,
                message="ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                current_task="í‚¤ì›Œë“œ ì¶”ì¶œ"
            )
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì „ì²˜ë¦¬
            processed_query = self.query_processor.preprocess_query(request.question)
            extracted_keywords = [keyword for keyword, weight in processed_query]
            
            yield ConciergeProgress(
                stage="keywords_extracted",
                progress=15,
                message=f"í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {', '.join(extracted_keywords[:5])}",
                current_task="ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½",
                extracted_keywords=extracted_keywords
            )
            
            # 2ë‹¨ê³„: ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½
            yield ConciergeProgress(
                stage="search_strategy",
                progress=25,
                message="ìµœì ì˜ ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                current_task="AND/OR ê²€ìƒ‰ ì „ëµ"
            )
            
            # ë‚ ì§œ ë²”ìœ„ ì„¤ì • - ìµœì‹ ì„± ê°•í™” (7ì¼ ìš°ì„ , 30ì¼ í´ë°±)
            date_from = request.date_from or (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")  # ìµœê·¼ 7ì¼
            date_to = request.date_to or (datetime.now() + timedelta(days=1)).strftime("%Y-%m-%d")
            
            # ê²€ìƒ‰ ì „ëµ êµ¬ì„±
            search_strategy = {
                "keywords": extracted_keywords,
                "date_range": f"{date_from} ~ {date_to}",
                "search_type": "AND_priority",  # AND ìš°ì„ , OR í´ë°±
                "max_articles": request.max_articles,
                "include_related_keywords": request.include_related_keywords,
                "include_today_issues": request.include_today_issues
            }
            
            yield ConciergeProgress(
                stage="search_strategy_ready",
                progress=35,
                message="ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½ ì™„ë£Œ. ë‰´ìŠ¤ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...",
                current_task="ë‰´ìŠ¤ ê²€ìƒ‰"
            )
            
            # 3ë‹¨ê³„: ë‰´ìŠ¤ ê²€ìƒ‰ (AND ìš°ì„ , OR í´ë°±)
            yield ConciergeProgress(
                stage="news_search",
                progress=45,
                message="ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                current_task="BigKinds API í˜¸ì¶œ"
            )
            
            # ê³ ê¸‰ ê²€ìƒ‰ ì‹¤í–‰ (10ê°œ ê¸°ì‚¬ ìš”ì²­)
            search_results = await self._execute_advanced_search(
                request.question, date_from, date_to, 10  # 10ê°œë¡œ í™•ì¥
            )
            
            articles = search_results.get("documents", [])
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ì¡°ê¸° ì¢…ë£Œ
            if not articles or len(articles) == 0 or search_results.get("search_failed", False):
                error_message = search_results.get("error_message", f"'{request.question}'ì— ëŒ€í•œ ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                yield ConciergeProgress(
                    stage="no_results",
                    progress=100,
                    message=error_message,
                    current_task="ê²€ìƒ‰ ì™„ë£Œ",
                    search_results_count=0
                )
                
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œì˜ ì‘ë‹µ ìƒì„±
                search_strategy = {
                    "keywords": extracted_keywords,
                    "date_range": f"{date_from} ~ {date_to}",
                    "search_type": "AND_priority",
                    "max_articles": request.max_articles,
                    "include_related_keywords": request.include_related_keywords,
                    "include_today_issues": request.include_today_issues
                }
                
                # í‚¤ì›Œë“œ ê¸°ë°˜ ì—°ê´€ì–´ ìƒì„± (ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ë„ ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” ì—°ê´€ì–´)
                related_keywords = []
                related_questions = []
                if extracted_keywords:
                    main_keyword = extracted_keywords[0]
                    related_keywords = self._generate_default_related_keywords(main_keyword)
                    
                    # ê¸°ë³¸ ê´€ë ¨ ì§ˆë¬¸ë„ ìƒì„±
                    if request.include_related_questions and (related_keywords or extracted_keywords):
                        available_keywords = related_keywords + extracted_keywords[:3]
                        keyword_weights = {kw: 1.0 - (i * 0.2) for i, kw in enumerate(available_keywords)}
                        
                        related_questions = self.questions_generator.generate_related_questions(
                            original_question=request.question,
                            related_keywords=available_keywords[:6],
                            keyword_weights=keyword_weights,
                            max_questions=4
                        )
                
                final_response = ConciergeResponse(
                    question=request.question,
                    answer=f"ì£„ì†¡í•©ë‹ˆë‹¤. '{request.question}'ì— ëŒ€í•œ ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\në‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ì„ ì‹œë„í•´ë³´ì„¸ìš”:\nâ€¢ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”\nâ€¢ ê²€ìƒ‰ ê¸°ê°„ì„ ì¡°ì •í•´ë³´ì„¸ìš”\nâ€¢ ë” ì¼ë°˜ì ì¸ ìš©ì–´ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”",
                    summary=f"'{request.question}' ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    key_points=[
                        "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”.",
                        "ê²€ìƒ‰ ê¸°ê°„ì„ ì¡°ì •í•´ë³´ì„¸ìš”.",
                        "ë” ì¼ë°˜ì ì¸ ìš©ì–´ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”."
                    ],
                    references=[],
                    related_keywords=related_keywords,
                    related_questions=related_questions,
                    today_issues=[],
                    search_strategy=search_strategy,
                    analysis_metadata={
                        "processing_time_seconds": round(time.time() - start_time, 2),
                        "articles_analyzed": 0,
                        "keywords_extracted": len(extracted_keywords),
                        "ai_model": "none",
                        "generated_at": datetime.now().isoformat(),
                        "error": "no_search_results",
                        "search_attempted": True,
                        "related_questions_count": len(related_questions)
                    },
                    generated_at=datetime.now().isoformat()
                )
                
                yield ConciergeProgress(
                    stage="completed",
                    progress=100,
                    message="ê²€ìƒ‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                    current_task="ì™„ë£Œ",
                    result=final_response
                )
                return
            
            yield ConciergeProgress(
                stage="search_completed",
                progress=55,
                message=f"{len(articles)}ê°œì˜ ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.",
                current_task="ì—°ê´€ì–´ ë° ì´ìŠˆ ìˆ˜ì§‘",
                search_results_count=len(articles)
            )
            
            # 4ë‹¨ê³„: ì—°ê´€ì–´ ë° ì˜¤ëŠ˜ì˜ ì´ìŠˆ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬ë¡œ ìµœì í™”)
            related_keywords = []
            today_issues = []
            
            yield ConciergeProgress(
                stage="parallel_collection",
                progress=65,
                message="ì—°ê´€ í‚¤ì›Œë“œì™€ ì˜¤ëŠ˜ì˜ ì´ìŠˆë¥¼ ë³‘ë ¬ë¡œ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                current_task="ë³‘ë ¬ API í˜¸ì¶œ"
            )
            
            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ íƒœìŠ¤í¬ ì¤€ë¹„
            tasks = []
            
            if request.include_related_keywords and extracted_keywords:
                main_keyword = extracted_keywords[0] if extracted_keywords else request.question
                tasks.append(self._get_related_keywords(main_keyword))
            else:
                tasks.append(asyncio.create_task(asyncio.sleep(0)))  # ë”ë¯¸ íƒœìŠ¤í¬
            
            if request.include_today_issues:
                tasks.append(self._get_today_issues())
            else:
                tasks.append(asyncio.create_task(asyncio.sleep(0)))  # ë”ë¯¸ íƒœìŠ¤í¬
            
            # ë³‘ë ¬ ì‹¤í–‰
            if tasks and len(tasks) >= 2:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # ì—°ê´€ì–´ ê²°ê³¼ ì²˜ë¦¬
                if request.include_related_keywords and extracted_keywords and len(results) > 0:
                    if not isinstance(results[0], Exception) and isinstance(results[0], list):
                        related_keywords = results[0]
                    else:
                        self.logger.warning(f"ì—°ê´€ì–´ ìˆ˜ì§‘ ì‹¤íŒ¨: {results[0] if len(results) > 0 else 'No results'}")
                        related_keywords = []
                
                # ì˜¤ëŠ˜ì˜ ì´ìŠˆ ê²°ê³¼ ì²˜ë¦¬
                if request.include_today_issues and len(results) > 1:
                    if not isinstance(results[1], Exception) and isinstance(results[1], list):
                        today_issues = results[1]
                    else:
                        self.logger.warning(f"ì˜¤ëŠ˜ì˜ ì´ìŠˆ ìˆ˜ì§‘ ì‹¤íŒ¨: {results[1] if len(results) > 1 else 'No results'}")
                        today_issues = []
            
            # 5ë‹¨ê³„: AI ë¶„ì„ ë° ë‹µë³€ ìƒì„±
            yield ConciergeProgress(
                stage="ai_analysis",
                progress=75,
                message="AIê°€ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ê³  ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                current_task="GPT-4 ë¶„ì„"
            )
            
            # ê¸°ì‚¬ ì°¸ì¡° ì •ë³´ ìƒì„±
            references = self._create_article_references(articles)
            
            # AI ë‹µë³€ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°)
            ai_response = await self._generate_ai_response_with_citations(
                request.question, articles, references, related_keywords, 
                today_issues, request.detail_level
            )
            
            # 6ë‹¨ê³„: ìµœì¢… ì‘ë‹µ êµ¬ì„±
            yield ConciergeProgress(
                stage="response_generation",
                progress=90,
                message="ìµœì¢… ë‹µë³€ì„ êµ¬ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                current_task="ì‘ë‹µ í¬ë§·íŒ…"
            )
            
            # ê²€ìƒ‰ ì „ëµ êµ¬ì„±    
            search_strategy = {
                "keywords": extracted_keywords,
                "date_range": f"{date_from} ~ {date_to}",
                "search_type": "AND_priority",
                "max_articles": request.max_articles,
                "include_related_keywords": request.include_related_keywords,
                "include_today_issues": request.include_today_issues
            }
            
            # ìµœì¢… ì‘ë‹µ ìƒì„±
            final_response = ConciergeResponse(
                question=request.question,
                answer=ai_response["answer"],
                summary=ai_response["summary"],
                key_points=ai_response["key_points"],
                references=references,
                related_keywords=related_keywords,
                related_questions=related_questions,
                today_issues=today_issues,
                search_strategy=search_strategy,
                analysis_metadata={
                    "processing_time_seconds": round(time.time() - start_time, 2),
                    "articles_analyzed": len(articles),
                    "keywords_extracted": len(extracted_keywords),
                    "ai_model": "gpt-4o-mini",  # ì‹¤ì œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ë¡œ ìˆ˜ì •
                    "generated_at": datetime.now().isoformat(),
                    "citations_used": ai_response.get("citations_used", []),
                    "total_citations": ai_response.get("total_citations", 0),
                    "related_keywords": related_keywords,
                    "related_questions_count": len(related_questions)
                },
                generated_at=datetime.now().isoformat()
            )
            
            # ì™„ë£Œ
            yield ConciergeProgress(
                stage="completed",
                progress=100,
                message="AI ë‰´ìŠ¤ ì»¨ì‹œì–´ì§€ ë‹µë³€ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!",
                current_task="ì™„ë£Œ",
                result=final_response
            )
            
        except Exception as e:
            self.logger.error(f"ì»¨ì‹œì–´ì§€ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            yield ConciergeProgress(
                stage="error",
                progress=0,
                message=f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                current_task="ì˜¤ë¥˜ ì²˜ë¦¬"
            )
    
    async def _execute_advanced_search(
        self, 
        question: str, 
        date_from: str, 
        date_to: str, 
        max_articles: int
    ) -> Dict[str, Any]:
        """ì§€ëŠ¥í˜• ë‹¤ë‹¨ê³„ ê²€ìƒ‰ ì „ëµ (ìš°ì„ ìˆœìœ„ ì•Œê³ ë¦¬ì¦˜ + í´ë°±)"""
        
        try:
            # í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¶„ì„
            processed_keywords = self.query_processor.preprocess_query(question)
            expanded_keywords = []
            for keyword, weight in processed_keywords:
                expanded_keywords.append(keyword)
                # ë™ì˜ì–´ í™•ì¥ - í¬ê´„ì  ì‚¬ì „
                synonyms = self._get_keyword_synonyms(keyword)
                expanded_keywords.extend(synonyms)
            
            unique_keywords = list(dict.fromkeys(expanded_keywords))
            
            self.logger.info(f"ì›ë³¸ ì§ˆë¬¸: {question}")
            self.logger.info(f"ì¶”ì¶œëœ í‚¤ì›Œë“œ: {unique_keywords}")
            
            # ë‹¤ë‹¨ê³„ ê²€ìƒ‰ ì „ëµ ì‹¤í–‰
            return await self._execute_multi_stage_search(
                unique_keywords, question, date_from, date_to, max_articles
            )
            
        except Exception as e:
            self.logger.error(f"ê³ ê¸‰ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ ê²€ìƒ‰ ì‹œë„
            try:
                fallback_result = self.bigkinds_client.search_news(
                    query=question,
                    date_from=date_from,
                    date_to=date_to,
                    return_size=max_articles
                )
                
                # í´ë°± ê²€ìƒ‰ ê²°ê³¼ë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
                fallback_return_object = fallback_result.get("return_object", {})
                fallback_documents = fallback_return_object.get("documents", [])
                fallback_total_hits = fallback_return_object.get("total_hits", 0)
                
                if not fallback_documents or len(fallback_documents) == 0:
                    return {
                        "documents": [],
                        "total_hits": 0,
                        "search_failed": True,
                        "error_message": f"'{question}'ì— ëŒ€í•œ ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”."
                    }
                
                return {
                    "documents": fallback_documents,
                    "total_hits": fallback_total_hits,
                    "search_failed": False,
                    "return_object": fallback_return_object
                }
                
            except Exception as fallback_error:
                self.logger.error(f"í´ë°± ê²€ìƒ‰ë„ ì‹¤íŒ¨: {fallback_error}")
                return {
                    "documents": [],
                    "total_hits": 0,
                    "search_failed": True,
                    "error_message": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(fallback_error)}"
                }
    
    async def _execute_multi_stage_search(
        self, 
        keywords: List[str], 
        question: str, 
        date_from: str, 
        date_to: str, 
        max_articles: int
    ) -> Dict[str, Any]:
        """ë‹¤ë‹¨ê³„ ê²€ìƒ‰ ì „ëµ ì‹¤í–‰ - AND ìš°ì„ ì—ì„œ ORë¡œ ì ì§„ì  í™•ì¥"""
        
        self.logger.info(f"ë‹¤ë‹¨ê³„ ê²€ìƒ‰ ì‹œì‘: í‚¤ì›Œë“œ={keywords[:5]}, ê¸°ê°„={date_from}~{date_to}")
        
        all_articles = []
        search_attempts = []
        
        # 1ë‹¨ê³„: í•µì‹¬ í‚¤ì›Œë“œë§Œìœ¼ë¡œ AND ê²€ìƒ‰ (ìµœê·¼ 7ì¼)
        if len(keywords) >= 2:
            core_keywords = keywords[:2]  # ìƒìœ„ 2ê°œ í•µì‹¬ í‚¤ì›Œë“œ
            query = " AND ".join(core_keywords)
            
            try:
                search_result = self.bigkinds_client.search_news(
                    query=query,
                    date_from=date_from,
                    date_to=date_to,
                    return_size=20
                )
                
                articles = search_result.get("return_object", {}).get("documents", [])
                
                if articles:
                    filtered_articles = self._filter_relevant_documents(articles, core_keywords, question)
                    new_articles = self._remove_duplicates(filtered_articles, all_articles)
                    
                    if new_articles:
                        self.logger.info(f"1ë‹¨ê³„ ì„±ê³µ: {len(new_articles)}ê°œ ê¸°ì‚¬ (í•µì‹¬ í‚¤ì›Œë“œ AND)")
                        all_articles.extend(new_articles[:max_articles-len(all_articles)])
                        search_attempts.append(f"1ë‹¨ê³„ ì„±ê³µ: {query} ({len(new_articles)}ê°œ)")
                        
                        if len(all_articles) >= max_articles:
                            return {
                                "documents": self._deduplicate_articles(all_articles[:max_articles]),
                                "total_hits": len(all_articles),
                                "search_failed": False
                            }
                    
            except Exception as e:
                self.logger.warning(f"1ë‹¨ê³„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                search_attempts.append(f"1ë‹¨ê³„ ì‹¤íŒ¨: {query}")
        
        # 2ë‹¨ê³„: í•µì‹¬ í‚¤ì›Œë“œ OR ê²€ìƒ‰ (ìµœê·¼ 7ì¼)
        if len(keywords) >= 2:
            core_keywords = keywords[:3]  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œ
            query = " OR ".join(core_keywords)
            
            try:
                search_result = self.bigkinds_client.search_news(
                    query=query,
                    date_from=date_from,
                    date_to=date_to,
                    return_size=30
                )
                
                articles = search_result.get("return_object", {}).get("documents", [])
                
                if articles:
                    filtered_articles = self._filter_relevant_documents(articles, core_keywords, question)
                    new_articles = self._remove_duplicates(filtered_articles, all_articles)
                    
                    if new_articles:
                        self.logger.info(f"2ë‹¨ê³„ ì„±ê³µ: {len(new_articles)}ê°œ ì¶”ê°€ ê¸°ì‚¬ (í•µì‹¬ í‚¤ì›Œë“œ OR)")
                        all_articles.extend(new_articles[:max_articles-len(all_articles)])
                        search_attempts.append(f"2ë‹¨ê³„ ì„±ê³µ: {query} ({len(new_articles)}ê°œ ì¶”ê°€)")
                        
                        if len(all_articles) >= max_articles:
                            return {
                                "documents": self._deduplicate_articles(all_articles[:max_articles]),
                                "total_hits": len(all_articles),
                                "search_failed": False
                            }
                    
            except Exception as e:
                self.logger.warning(f"2ë‹¨ê³„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                search_attempts.append(f"2ë‹¨ê³„ ì‹¤íŒ¨: {query}")
        
        # 3ë‹¨ê³„: ë‚ ì§œ ë²”ìœ„ í™•ì¥ (30ì¼) + í•µì‹¬ í‚¤ì›Œë“œ AND
        if len(all_articles) < max_articles // 2:  # ì¶©ë¶„í•œ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ë‚ ì§œ í™•ì¥
            from datetime import datetime, timedelta
            extended_date_from = (datetime.strptime(date_to, "%Y-%m-%d") - timedelta(days=30)).strftime("%Y-%m-%d")
            
            if len(keywords) >= 2:
                core_keywords = keywords[:2]
                query = " AND ".join(core_keywords)
                
                try:
                    search_result = self.bigkinds_client.search_news(
                        query=query,
                        date_from=extended_date_from,
                        date_to=date_to,
                        return_size=25
                    )
                    
                    articles = search_result.get("return_object", {}).get("documents", [])
                    
                    if articles:
                        filtered_articles = self._filter_relevant_documents(articles, core_keywords, question)
                        new_articles = self._remove_duplicates(filtered_articles, all_articles)
                        
                        if new_articles:
                            self.logger.info(f"3ë‹¨ê³„ ì„±ê³µ: {len(new_articles)}ê°œ ì¶”ê°€ ê¸°ì‚¬ (30ì¼ í™•ì¥ + AND)")
                            all_articles.extend(new_articles[:max_articles-len(all_articles)])
                            search_attempts.append(f"3ë‹¨ê³„ ì„±ê³µ: {query} (30ì¼, {len(new_articles)}ê°œ ì¶”ê°€)")
                            
                            if len(all_articles) >= max_articles:
                                return {
                                    "documents": self._deduplicate_articles(all_articles[:max_articles]),
                                    "total_hits": len(all_articles),
                                    "search_failed": False
                                }
                        
                except Exception as e:
                    self.logger.warning(f"3ë‹¨ê³„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    search_attempts.append(f"3ë‹¨ê³„ ì‹¤íŒ¨: {query} (30ì¼)")
        
        # 4ë‹¨ê³„: ë‚ ì§œ ë²”ìœ„ í™•ì¥ (30ì¼) + í•µì‹¬ í‚¤ì›Œë“œ OR
        if len(all_articles) < max_articles // 2:
            from datetime import datetime, timedelta
            extended_date_from = (datetime.strptime(date_to, "%Y-%m-%d") - timedelta(days=30)).strftime("%Y-%m-%d")
            
            if len(keywords) >= 1:
                core_keywords = keywords[:3]
                query = " OR ".join(core_keywords)
                
                try:
                    search_result = self.bigkinds_client.search_news(
                        query=query,
                        date_from=extended_date_from,
                        date_to=date_to,
                        return_size=30
                    )
                    
                    articles = search_result.get("return_object", {}).get("documents", [])
                    
                    if articles:
                        filtered_articles = self._filter_relevant_documents(articles, core_keywords, question)
                        new_articles = self._remove_duplicates(filtered_articles, all_articles)
                        
                        if new_articles:
                            self.logger.info(f"4ë‹¨ê³„ ì„±ê³µ: {len(new_articles)}ê°œ ì¶”ê°€ ê¸°ì‚¬ (30ì¼ í™•ì¥ + OR)")
                            all_articles.extend(new_articles[:max_articles-len(all_articles)])
                            search_attempts.append(f"4ë‹¨ê³„ ì„±ê³µ: {query} (30ì¼, {len(new_articles)}ê°œ ì¶”ê°€)")
                            
                            if len(all_articles) >= max_articles:
                                return {
                                    "documents": self._deduplicate_articles(all_articles[:max_articles]),
                                    "total_hits": len(all_articles),
                                    "search_failed": False
                                }
                        
                except Exception as e:
                    self.logger.warning(f"4ë‹¨ê³„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    search_attempts.append(f"4ë‹¨ê³„ ì‹¤íŒ¨: {query} (30ì¼)")
        
        # 5ë‹¨ê³„: ìµœí›„ì˜ ìˆ˜ë‹¨ - ì²« ë²ˆì§¸ í‚¤ì›Œë“œë§Œìœ¼ë¡œ 90ì¼ ê²€ìƒ‰
        if len(all_articles) < 3 and keywords:
            from datetime import datetime, timedelta
            extended_date_from = (datetime.strptime(date_to, "%Y-%m-%d") - timedelta(days=90)).strftime("%Y-%m-%d")
            
            query = keywords[0]  # ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ í•˜ë‚˜ë§Œ
            
            try:
                search_result = self.bigkinds_client.search_news(
                    query=query,
                    date_from=extended_date_from,
                    date_to=date_to,
                    return_size=20
                )
                
                articles = search_result.get("return_object", {}).get("documents", [])
                
                if articles:
                    new_articles = self._remove_duplicates(articles, all_articles)
                    
                    if new_articles:
                        self.logger.info(f"5ë‹¨ê³„ ì„±ê³µ: {len(new_articles)}ê°œ ì¶”ê°€ ê¸°ì‚¬ (90ì¼ + ë‹¨ì¼ í‚¤ì›Œë“œ)")
                        all_articles.extend(new_articles[:max_articles-len(all_articles)])
                        search_attempts.append(f"5ë‹¨ê³„ ì„±ê³µ: {query} (90ì¼, {len(new_articles)}ê°œ ì¶”ê°€)")
                    
            except Exception as e:
                self.logger.warning(f"5ë‹¨ê³„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                search_attempts.append(f"5ë‹¨ê³„ ì‹¤íŒ¨: {query} (90ì¼)")
        
        # ê²€ìƒ‰ ì‹œë„ ë¡œê·¸ ì¶œë ¥
        self.logger.info(f"ë‹¤ë‹¨ê³„ ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(all_articles)}ê°œ ê¸°ì‚¬")
        for attempt in search_attempts:
            self.logger.info(f"  - {attempt}")
        
        # ìµœì¢… ì¤‘ë³µ ì œê±° ë° ê´€ë ¨ì„± ìˆœ ì •ë ¬
        final_articles = self._deduplicate_articles(all_articles)
        
        if not final_articles:
            self.logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ. í‚¤ì›Œë“œ: {keywords}, ì§ˆë¬¸: {question}")
            return {
                "documents": [],
                "total_hits": 0,
                "search_failed": True,
                "error_message": f"'{question}'ì— ëŒ€í•œ ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }
        
        return {
            "documents": final_articles[:max_articles],
            "total_hits": len(final_articles),
            "search_failed": False
        }
    
    def _remove_duplicates(self, new_articles: List[Dict[str, Any]], existing_articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ìƒˆë¡œìš´ ê¸°ì‚¬ ëª©ë¡ì—ì„œ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê¸°ì‚¬ë“¤ì„ ì œê±°"""
        existing_ids = {article.get("news_id", "") for article in existing_articles}
        return [article for article in new_articles if article.get("news_id", "") not in existing_ids]
    
    def _deduplicate_articles(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ê¸°ì‚¬ ëª©ë¡ì—ì„œ ì¤‘ë³µ ì œê±°"""
        seen_ids = set()
        deduplicated = []
        
        for article in articles:
            news_id = article.get("news_id", "")
            if news_id and news_id not in seen_ids:
                seen_ids.add(news_id)
                deduplicated.append(article)
        
        return deduplicated
    
    async def _get_related_keywords(self, keyword: str, max_count: int = 10) -> List[str]:
        """ì—°ê´€ í‚¤ì›Œë“œ ìˆ˜ì§‘"""
        try:
            # BigKinds ì—°ê´€ì–´ API í˜¸ì¶œ
            related_data = self.bigkinds_client.get_related_keywords(keyword, max_count)
            if isinstance(related_data, list) and len(related_data) > 0:
                print(f"DEBUG: BigKindsì—ì„œ ìˆ˜ì§‘ëœ ì—°ê´€ì–´: {related_data}")
                return related_data
            else:
                print(f"DEBUG: BigKinds ì—°ê´€ì–´ API ì‘ë‹µì´ ë¹„ì–´ìˆìŒ, ê¸°ë³¸ ì—°ê´€ì–´ ìƒì„±")
                # BigKinds APIê°€ ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ ì—°ê´€ì–´ ìƒì„±
                return self._generate_default_related_keywords(keyword)
        except Exception as e:
            self.logger.error(f"ì—°ê´€ì–´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            print(f"DEBUG: ì—°ê´€ì–´ ìˆ˜ì§‘ ì‹¤íŒ¨, ê¸°ë³¸ ì—°ê´€ì–´ ìƒì„±: {e}")
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ì—°ê´€ì–´ ìƒì„±
            return self._generate_default_related_keywords(keyword)
    
    def _generate_default_related_keywords(self, keyword: str) -> List[str]:
        """ê¸°ë³¸ ì—°ê´€ì–´ ìƒì„± - í‚¤ì›Œë“œ ê¸°ë°˜"""
        
        # í‚¤ì›Œë“œë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ë§¤ì¹­
        keyword_lower = keyword.lower()
        
        # í‚¤ì›Œë“œë³„ ê´€ë ¨ì–´ ë§¤í•‘
        keyword_mappings = {
            # ê¸°ìˆ /IT ê´€ë ¨
            "ai": ["ì¸ê³µì§€ëŠ¥", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "ë¹…ë°ì´í„°", "ì•Œê³ ë¦¬ì¦˜", "ChatGPT", "ìƒì„±AI"],
            "ì¸ê³µì§€ëŠ¥": ["AI", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "ë¹…ë°ì´í„°", "ì•Œê³ ë¦¬ì¦˜", "ChatGPT", "ìƒì„±AI"],
            "ë°˜ë„ì²´": ["ì¹©", "ë©”ëª¨ë¦¬", "ì‹œìŠ¤í…œë°˜ë„ì²´", "íŒŒìš´ë“œë¦¬", "ì›¨ì´í¼", "TSMC", "ì‚¼ì„±ì „ì"],
            "gpu": ["NVIDIA", "AMD", "ê·¸ë˜í”½ì¹´ë“œ", "AIì¹©", "ë³‘ë ¬ì²˜ë¦¬", "CUDA"],
            "nvidia": ["GPU", "AIì¹©", "ê·¸ë˜í”½ì¹´ë“œ", "CUDA", "ë°ì´í„°ì„¼í„°", "ì  ìŠ¨í™©"],
            "ì‚¼ì„±": ["ê°¤ëŸ­ì‹œ", "ë©”ëª¨ë¦¬", "ë””ìŠ¤í”Œë ˆì´", "ë°˜ë„ì²´", "ìŠ¤ë§ˆíŠ¸í°", "ì´ì¬ìš©"],
            
            # êµ­ì œ/ì •ì¹˜ ê´€ë ¨
            "ì´ë€": ["í•µ", "ì œì¬", "ì¤‘ë™", "ì„ìœ ", "IAEA", "ìš°ë¼ëŠ„", "í•µì‹œì„¤", "í…Œí—¤ë€"],
            "í•µ": ["ì›ìë ¥", "ìš°ë¼ëŠ„", "í•µë°œì „", "í•µë¬´ê¸°", "ì›ì „", "IAEA", "í•µì‹œì„¤"],
            "í•µì‹œì„¤": ["ì›ìë ¥", "ìš°ë¼ëŠ„", "í•µë°œì „", "ì›ì „", "IAEA", "í•µë¬´ê¸°", "ë°©ì‚¬ëŠ¥"],
            "ë¯¸êµ­": ["íŠ¸ëŸ¼í”„", "ë°”ì´ë“ ", "ë‹¬ëŸ¬", "ì—°ì¤€", "ê²½ì œ", "ë°±ì•…ê´€", "êµ­ë¬´ë¶€"],
            "ì¤‘êµ­": ["ì‹œì§„í•‘", "ë¬´ì—­", "ê²½ì œ", "í™ì½©", "ëŒ€ë§Œ", "ë² ì´ì§•", "ìœ„ì•ˆí™”"],
            "ëŸ¬ì‹œì•„": ["í‘¸í‹´", "ìš°í¬ë¼ì´ë‚˜", "ì²œì—°ê°€ìŠ¤", "ë£¨ë¸”", "ëª¨ìŠ¤í¬ë°”", "ì œì¬"],
            "ì¼ë³¸": ["ê¸°ì‹œë‹¤", "ì—”í™”", "ë„ì¿„", "í›„ì¿ ì‹œë§ˆ", "ì›ì „", "ê²½ì œ"],
            
            # ê²½ì œ ê´€ë ¨
            "ê²½ì œ": ["GDP", "ì¸í”Œë ˆì´ì…˜", "ê¸ˆë¦¬", "ì£¼ì‹", "í™˜ìœ¨", "ì„±ì¥ë¥ ", "ê²½ê¸°"],
            "ì£¼ì‹": ["ì½”ìŠ¤í”¼", "ë‚˜ìŠ¤ë‹¥", "ë‹¤ìš°", "íˆ¬ì", "ì¦ì‹œ", "ìƒì¥", "ë°°ë‹¹"],
            "ë¶€ë™ì‚°": ["ì•„íŒŒíŠ¸", "ì „ì„¸", "ë§¤ë§¤", "ëŒ€ì¶œ", "ì •ì±…", "ì§‘ê°’", "ì„ëŒ€"],
            "ê¸ˆë¦¬": ["ê¸°ì¤€ê¸ˆë¦¬", "ëŒ€ì¶œê¸ˆë¦¬", "ì˜ˆê¸ˆê¸ˆë¦¬", "ì¸í”Œë ˆì´ì…˜", "ì¤‘ì•™ì€í–‰"],
            "ì¸í”Œë ˆì´ì…˜": ["ë¬¼ê°€", "ì†Œë¹„ìë¬¼ê°€", "ê¸ˆë¦¬", "ê²½ì œ", "ì¤‘ì•™ì€í–‰"],
            
            # ì—ë„ˆì§€/í™˜ê²½ ê´€ë ¨
            "ê¸°í›„": ["ì˜¨ì‹¤ê°€ìŠ¤", "íƒ„ì†Œì¤‘ë¦½", "ì‹ ì¬ìƒì—ë„ˆì§€", "í™˜ê²½", "ì§€êµ¬ì˜¨ë‚œí™”", "íŒŒë¦¬í˜‘ì •"],
            "ì›ì „": ["ì›ìë ¥", "í•µë°œì „", "ë°©ì‚¬ëŠ¥", "ìš°ë¼ëŠ„", "í›„ì¿ ì‹œë§ˆ", "ì²´ë¥´ë…¸ë¹Œ"],
            "ì„ìœ ": ["ì›ìœ ", "ê°€ê²©", "OPEC", "ì •ì œ", "ì—ë„ˆì§€", "ë°°ëŸ´"],
            
            # ê¸°íƒ€
            "ì½”ë¡œë‚˜": ["ë°±ì‹ ", "í™•ì§„", "ë°©ì—­", "WHO", "íŒ¬ë°ë¯¹", "ë³€ì´", "ì¹˜ë£Œì œ"],
            "ë¶í•œ": ["ê¹€ì •ì€", "í•µ", "ë¯¸ì‚¬ì¼", "ì œì¬", "í‰ì–‘", "ë¹„í•µí™”"],
            "ìš°í¬ë¼ì´ë‚˜": ["ëŸ¬ì‹œì•„", "ì „ìŸ", "ì ¤ë ŒìŠ¤í‚¤", "í‘¸í‹´", "í‚¤ì˜ˆí”„", "NATO"]
        }
        
        # í‚¤ì›Œë“œì™€ ë§¤ì¹­ë˜ëŠ” ì—°ê´€ì–´ ì°¾ê¸°
        related_keywords = []
        
        # ì§ì ‘ ë§¤ì¹­ ì‹œë„
        for key, values in keyword_mappings.items():
            if key in keyword_lower:
                related_keywords.extend(values)
                break
        
        # ì§ì ‘ ë§¤ì¹­ì´ ì•ˆë˜ë©´ ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
        if not related_keywords:
            for key, values in keyword_mappings.items():
                if any(key in word for word in keyword_lower.split()) or any(word in key for word in keyword_lower.split()):
                    related_keywords.extend(values)
                    break
        
        # ì—¬ì „íˆ ë§¤ì¹­ë˜ì§€ ì•Šìœ¼ë©´ í‚¤ì›Œë“œì—ì„œ ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ ì¶”ì¶œ
        if not related_keywords:
            words = keyword_lower.split()
            # 2ê¸€ì ì´ìƒì˜ ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ë“¤ë§Œ ì¶”ì¶œ
            meaningful_words = [word for word in words if len(word) >= 2 and word not in ['ê·¸ë¦¬ê³ ', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ë˜ì„œ', 'ë•Œë¬¸ì—']]
            if meaningful_words:
                related_keywords = meaningful_words[:5]
        
        # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ê¸°ë³¸ AI ë°˜ë„ì²´ ì—°ê´€ì–´ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        if not related_keywords:
            self.logger.info(f"í‚¤ì›Œë“œ '{keyword}'ì— ëŒ€í•œ ì ì ˆí•œ ì—°ê´€ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # ì¤‘ë³µ ì œê±°í•˜ê³  ìµœëŒ€ 8ê°œê¹Œì§€ ë°˜í™˜
        unique_keywords = list(set(related_keywords))
        return unique_keywords[:8]
    
    async def _get_today_issues(self) -> List[Dict[str, Any]]:
        """ì˜¤ëŠ˜ì˜ ì´ìŠˆ ìˆ˜ì§‘"""
        try:
            # BigKinds ì´ìŠˆ ë­í‚¹ API í˜¸ì¶œ
            issues_data = self.bigkinds_client.get_issue_ranking(
                date=datetime.now().strftime("%Y-%m-%d")
            )
            return issues_data.get("issues", []) if isinstance(issues_data, dict) else []
        except Exception as e:
            self.logger.error(f"ì˜¤ëŠ˜ì˜ ì´ìŠˆ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def _create_article_references(self, articles: List[Dict[str, Any]]) -> List[ArticleReference]:
        """ê¸°ì‚¬ ì°¸ì¡° ì •ë³´ ìƒì„±"""
        references = []
        
        for i, article in enumerate(articles):
            ref_id = f"ref{i+1}"
            
            # URL í•„ë“œ ì²˜ë¦¬ - BigKinds APIëŠ” provider_link_page í•„ë“œ ì‚¬ìš©
            article_url = article.get("url") or article.get("provider_link_page", "")
            
            reference = ArticleReference(
                ref_id=ref_id,
                title=article.get("title", "ì œëª© ì—†ìŒ"),
                provider=article.get("provider", article.get("provider_name", "ì–¸ë¡ ì‚¬ ì •ë³´ ì—†ìŒ")),
                published_at=article.get("published_at", "ë‚ ì§œ ì •ë³´ ì—†ìŒ"),
                url=article_url,
                relevance_score=article.get("_score", 0.0) / 100.0  # ì •ê·œí™”
            )
            references.append(reference)
        
        return references
    
    async def _generate_ai_response_with_citations(
        self,
        question: str,
        articles: List[Dict[str, Any]],
        references: List[ArticleReference],
        related_keywords: List[str],
        today_issues: List[Dict[str, Any]],
        detail_level: str
    ) -> Dict[str, Any]:
        """ê°ì£¼ í¬í•¨ AI ì‘ë‹µ ìƒì„± - ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš©ë§Œ ì‚¬ìš©"""
        
        # 10ê°œ ê¸°ì‚¬ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ë¶„ì„
        top_articles = articles[:10]
        top_references = references[:10]
        
        # ê¸°ì‚¬ ë‚´ìš©ê³¼ í‚¤ì›Œë“œ ë§¤ì¹­ ê²€ì¦
        verified_articles = self._verify_article_relevance(top_articles, question, related_keywords)
        
        if not verified_articles:
            self.logger.warning("ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. '{question}'ì™€ ê´€ë ¨ëœ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë‚˜ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "summary": "ê´€ë ¨ ê¸°ì‚¬ ì—†ìŒ",
                "key_points": ["ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ìµœê·¼ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."],
                "citations_used": [],
                "related_keywords": related_keywords
            }
        
        # ê¸°ì‚¬ ë‚´ìš© êµ¬ì„± (í•˜ì´ë¼ì´íŠ¸ ì •ë³´ ìš°ì„  í™œìš©)
        articles_text = ""
        for i, article in enumerate(verified_articles):
            ref_id = f"ref{i+1}"
            title = article.get("title", "")
            content = article.get("content", article.get("summary", ""))[:1000]  # ë” ë§ì€ ë‚´ìš© í¬í•¨
            provider = article.get("provider", "")
            published_at = article.get("published_at", "")
            
            # í•˜ì´ë¼ì´íŠ¸ ì •ë³´ ì¶”ì¶œ ë° ê²€ì¦
            highlight_info = article.get("highlight", {})
            highlighted_sentences = []
            
            # í•˜ì´ë¼ì´íŠ¸ëœ ì œëª©ê³¼ ë‚´ìš© ì¶”ì¶œ
            if highlight_info:
                if "title" in highlight_info and highlight_info["title"]:
                    highlighted_sentences.extend(highlight_info["title"])
                if "content" in highlight_info and highlight_info["content"]:
                    # ìƒìœ„ 8ê°œ ë¬¸ì¥ìœ¼ë¡œ í™•ëŒ€
                    highlighted_sentences.extend(highlight_info["content"][:8])
            
            articles_text += f"\n[{ref_id}] ì œëª©: {title}\n"
            articles_text += f"ì–¸ë¡ ì‚¬: {provider} | ë°œí–‰ì¼: {published_at}\n"
            articles_text += f"ë‚´ìš©: {content}\n"
            
            # í•˜ì´ë¼ì´íŠ¸ëœ í•µì‹¬ ë¬¸ì¥ ì¶”ê°€ (ì¤‘ìš”!)
            if highlighted_sentences:
                articles_text += f"**í•µì‹¬ ë¬¸ì¥ (í‚¤ì›Œë“œ ë§¤ì¹­)**: {' | '.join(highlighted_sentences[:8])}\n"
            
            articles_text += f"---\n"
        
        # GPT-4 í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ê°ì£¼ ì‹œìŠ¤í…œ ê°•í™” - ìì—°ìŠ¤ëŸ¬ìš´ íë¦„)
        system_prompt = """ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê°ê´€ì ì´ê³  í†µì°°ë ¥ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

â˜…â˜…â˜… í•µì‹¬ ê·œì¹™: ëª¨ë“  ë¬¸ì¥ì€ ë°˜ë“œì‹œ ì¸ìš© ë²ˆí˜¸(1~10)ë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤ â˜…â˜…â˜…

ë‹µë³€ ì‘ì„± ê·œì¹™:
1. ë°˜ë“œì‹œ ì œê³µëœ ê¸°ì‚¬ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ëª¨ë“  ë¬¸ì¥ì˜ ëì— ì¸ìš© ë²ˆí˜¸ë¥¼ í‘œì‹œí•˜ì„¸ìš” (ì˜ˆ: ë¬¸ì¥ ëì— 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
3. ì¸ìš© ë²ˆí˜¸ëŠ” ë¬¸ì¥ë¶€í˜¸ ë°”ë¡œ ë’¤ì— ê³µë°± ì—†ì´ ìˆ«ìë§Œ í‘œì‹œ
4. ì˜¬ë°”ë¥¸ ì˜ˆ: "ë°œí‘œí–ˆë‹¤1", "ì¦ê°€í–ˆë‹¤2", "ì˜ˆì •ì´ë‹¤3", "ë¶„ì„í–ˆë‹¤8", "ì „ë§ì´ë‹¤10"
5. ì˜ëª»ëœ ì˜ˆ: "ë°œí‘œí–ˆë‹¤ 1", "ë°œí‘œí–ˆë‹¤[1]", "ë°œí‘œí–ˆë‹¤(1)"
6. ì¶”ì¸¡ì´ë‚˜ ê°œì¸ì  ì˜ê²¬ë³´ë‹¤ëŠ” ê¸°ì‚¬ì— ë‚˜íƒ€ë‚œ ì‚¬ì‹¤ê³¼ ë°ì´í„°ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”
7. ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰¬ìš´ íë¦„ìœ¼ë¡œ ì‘ì„±í•˜ë˜, ì •ë³´ì˜ ì¶œì²˜ë¥¼ ëª…í™•íˆ í•´ì£¼ì„¸ìš”
8. ê¸°ì‚¬ì—ì„œ ì¸ìš©í•œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ë°œì–¸ì´ ìˆë‹¤ë©´ ë¬¸ì¥ ëì— í•´ë‹¹ ê¸°ì‚¬ ë²ˆí˜¸ë¥¼ í‘œì‹œí•˜ì„¸ìš”
9. í•œ ë¬¸ì¥ì— ì—¬ëŸ¬ ê¸°ì‚¬ì˜ ì •ë³´ê°€ ìˆìœ¼ë©´ ê°€ì¥ ì¤‘ìš”í•œ ì¶œì²˜ í•˜ë‚˜ë§Œ í‘œì‹œ

â˜…â˜…â˜… ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸ ì‘ì„± ê·œì¹™ â˜…â˜…â˜…:
- ë¬¸ë‹¨ì„ ì ì ˆíˆ ë‚˜ëˆ„ì–´ ì‘ì„±: ì£¼ì œê°€ ë°”ë€” ë•Œë§ˆë‹¤ ë¹ˆ ì¤„ë¡œ ë¬¸ë‹¨ì„ êµ¬ë¶„í•˜ì„¸ìš”
- ìì—°ìŠ¤ëŸ¬ìš´ íë¦„: ì´ì•¼ê¸°í•˜ë“¯ ìì—°ìŠ¤ëŸ½ê²Œ ì •ë³´ë¥¼ ì „ë‹¬í•˜ì„¸ìš”
- ì†Œì œëª©ì´ë‚˜ íŠ¹ìˆ˜ê¸°í˜¸ ì‚¬ìš© ê¸ˆì§€: **, ##, ğŸ“– ë“± ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- ê¹”ë”í•œ í…ìŠ¤íŠ¸: ìˆœìˆ˜í•œ í…ìŠ¤íŠ¸ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”
- ì ì ˆí•œ ë¬¸ì¥ ê¸¸ì´: í•œ ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ì§€ ì•Šë„ë¡ ì ì ˆíˆ ë‚˜ëˆ„ì–´ ì‘ì„±

êµ¬ì²´ì  ì •ë³´ í¬í•¨ ì˜ë¬´ (ë§¤ìš° ì¤‘ìš”):
- ì¸ëª…: ê´€ë ¨ëœ ëª¨ë“  ì¸ë¬¼ì˜ ì‹¤ëª…ê³¼ ì§ì±…ì„ ì •í™•íˆ ëª…ì‹œ (ì˜ˆ: "í™ê¸¸ë™ ê¸ˆìœµìœ„ì›ì¥ì— ë”°ë¥´ë©´")
- ì§€ëª…: êµ¬ì²´ì ì¸ ì§€ì—­ëª…, êµ­ê°€ëª…, ë„ì‹œëª… ë“±ì„ ëª…í™•íˆ í‘œê¸°
- ë‚ ì§œì™€ ì‹œê°„: êµ¬ì²´ì ì¸ ë‚ ì§œ, ì‹œê°„, ê¸°ê°„, ì‹œì ì„ ì •í™•íˆ ê¸°ì¬ (ì˜ˆ: "7ì›” 5ì¼ ì˜¤ì „ 9ì‹œ ë°œí‘œì— ë”°ë¥´ë©´")
- ê¸°ê´€ëª…: ê´€ë ¨ ê¸°ê´€, íšŒì‚¬, ì¡°ì§ì˜ ì •í™•í•œ ëª…ì¹­ í¬í•¨ (ì˜ˆ: "ì„œìš¸ê²½ì œì‹ ë¬¸ì— ë”°ë¥´ë©´")
- ìˆ˜ì¹˜: ê¸ˆì•¡, ë¹„ìœ¨, ê·œëª¨ ë“± êµ¬ì²´ì  ìˆ˜ì¹˜ ë°˜ë“œì‹œ í¬í•¨
- ì›ë¬¸ ì¸ìš©: ê¸°ì‚¬ ì›ë¬¸ì˜ ì¶œì²˜ë‚˜ ì¸ìš© ë¬¸êµ¬ í¬í•¨ (ì˜ˆ: "~ì— ë”°ë¥´ë©´", "~ë¼ê³  ë°í˜”ë‹¤")
- ê¸°ì‚¬ì— ì—†ëŠ” ë‚´ìš©ì´ë‚˜ ì¶”ì¸¡ì„± ë‚´ìš© ì¶”ê°€ ê¸ˆì§€"""

        user_prompt = f"""ì§ˆë¬¸: {question}

{response_instruction}ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¶„ì„í•  ê¸°ì‚¬ë“¤ (ë°˜ë“œì‹œ ì´ ê¸°ì‚¬ë“¤ì˜ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”):
{articles_text}

{related_text}

{issues_text}

ìœ„ 10ê°œ ê¸°ì‚¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. 

â˜…â˜…â˜… ì¸ìš© ë²ˆí˜¸ í‘œì‹œ í•„ìˆ˜ ê·œì¹™ (ë§¤ìš° ì¤‘ìš”) â˜…â˜…â˜…:
1. ëª¨ë“  ë¬¸ì¥ì€ ë°˜ë“œì‹œ ì¸ìš© ë²ˆí˜¸ë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤
2. ì¸ìš© ë²ˆí˜¸ëŠ” ë¬¸ì¥ë¶€í˜¸(ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ) ë°”ë¡œ ë’¤ì— ê³µë°± ì—†ì´ ìˆ«ìë§Œ í‘œì‹œ
3. ì˜¬ë°”ë¥¸ í˜•ì‹: "ë°œí‘œí–ˆë‹¤1", "ì¦ê°€í–ˆë‹¤2", "ì˜ˆì •ì´ë‹¤3"
4. ì˜ëª»ëœ í˜•ì‹: "ë°œí‘œí–ˆë‹¤ 1", "ë°œí‘œí–ˆë‹¤[1]", "ë°œí‘œí–ˆë‹¤(1)"
5. í•œ ë¬¸ì¥ì— ì—¬ëŸ¬ ê¸°ì‚¬ ì •ë³´ê°€ ìˆìœ¼ë©´ ì£¼ìš” ì¶œì²˜ í•˜ë‚˜ë§Œ í‘œì‹œ

â˜…â˜…â˜… ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ì‘ì„± ê·œì¹™ â˜…â˜…â˜…:
1. ë¬¸ë‹¨ êµ¬ë¶„: ì£¼ì œê°€ ë°”ë€” ë•Œë§ˆë‹¤ ë°˜ë“œì‹œ ë¹ˆ ì¤„ë¡œ ë¬¸ë‹¨ì„ ë‚˜ëˆ„ì„¸ìš”
2. ìì—°ìŠ¤ëŸ¬ìš´ íë¦„: ì´ì•¼ê¸°í•˜ë“¯ ìì—°ìŠ¤ëŸ½ê²Œ ì •ë³´ë¥¼ ì „ë‹¬í•˜ì„¸ìš”
3. íŠ¹ìˆ˜ê¸°í˜¸ ì‚¬ìš© ê¸ˆì§€: **, ##, ğŸ“– ë“± íŠ¹ìˆ˜ê¸°í˜¸ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
4. ê¹”ë”í•œ í…ìŠ¤íŠ¸: ìˆœìˆ˜í•œ í…ìŠ¤íŠ¸ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”
5. ì ì ˆí•œ ë¬¸ì¥ ê¸¸ì´: í•œ ë¬¸ì¥ì´ 3ì¤„ì„ ë„˜ì§€ ì•Šë„ë¡ ì¡°ì ˆí•˜ì—¬ ì½ê¸° ì‰½ê²Œ ì‘ì„±

ì¸ìš© ë²ˆí˜¸ ì˜ˆì‹œ:
- "ì‚¼ì„±ì „ìëŠ” ì˜¬í•´ HBM ë§¤ì¶œì´ ì „ë…„ ëŒ€ë¹„ 50% ì¦ê°€í–ˆë‹¤ê³  ë°œí‘œí–ˆë‹¤1"

(ë¬¸ë‹¨ ë‚˜ëˆ„ê¸°)

- "AI ë°˜ë„ì²´ ìˆ˜ìš” ê¸‰ì¦ìœ¼ë¡œ í–¥í›„ ì „ë§ë„ ë°ì€ ê²ƒìœ¼ë¡œ ë¶„ì„ëœë‹¤2"

ì¶”ê°€ ì§€ì¹¨:
- ê° ë¬¸ì¥ì´ë‚˜ ì •ë³´ì˜ ëì— í•´ë‹¹ ê¸°ì‚¬ ë²ˆí˜¸ë¥¼ í‘œì‹œí•˜ì„¸ìš” (1, 2, 3, 4, 5)
- ê¸°ì‚¬ì— ë‚˜ì˜¨ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ë°œì–¸, ê³„íš ë“±ì„ ì¸ìš©í•  ë•ŒëŠ” í•´ë‹¹ ë¬¸ì¥ ëì— ê¸°ì‚¬ ë²ˆí˜¸ë¥¼ í‘œì‹œí•˜ì„¸ìš”
- ì—¬ëŸ¬ ê¸°ì‚¬ì—ì„œ ë¹„ìŠ·í•œ ë‚´ìš©ì´ ë‚˜ì˜¨ë‹¤ë©´ ì ì ˆí•œ ê¸°ì‚¬ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì—¬ í‘œì‹œí•˜ì„¸ìš”
- ê¸°ì‚¬ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”

êµ¬ì²´ì  ì •ë³´ í•„ìˆ˜ í¬í•¨ì‚¬í•­:
- ì¸ë¬¼ ì–¸ê¸‰ ì‹œ: "í™ê¸¸ë™ XXíšŒì‚¬ ëŒ€í‘œ", "ê¹€ì² ìˆ˜ ê¸ˆìœµìœ„ì›ì¥" ë“± ì‹¤ëª…+ì§ì±… ëª…ì‹œ
- ì‹œê°„ ì •ë³´: "7ì›” 5ì¼", "ì˜¤ì „ 9ì‹œ", "2024ë…„ ìƒë°˜ê¸°" ë“± êµ¬ì²´ì  ì‹œì  í‘œê¸°
- ì¥ì†Œ ì •ë³´: "ì„œìš¸ ê°•ë‚¨êµ¬", "ë¯¸êµ­ ë‰´ìš•", "ì¤‘êµ­ ë² ì´ì§•" ë“± êµ¬ì²´ì  ì§€ëª…
- ê¸°ê´€ëª…: "ì‚¼ì„±ì „ì", "ê¸ˆìœµìœ„ì›íšŒ", "í•œêµ­ì€í–‰" ë“± ì •í™•í•œ ê¸°ê´€ëª…
- ìˆ˜ì¹˜ ë°ì´í„°: "30% ì¦ê°€", "1ì¡°ì› ê·œëª¨", "500ë§Œ ë‹¬ëŸ¬" ë“± êµ¬ì²´ì  ìˆ˜ì¹˜
- ì¸ìš©êµ¬: "~ë¼ê³  ë§í–ˆë‹¤", "~ì— ë”°ë¥´ë©´", "~ë¡œ ì „í•´ì¡Œë‹¤" ë“± ì›ë¬¸ í‘œí˜„ í™œìš©"""

        try:
            # GPT-4 API í˜¸ì¶œ (ë¹„ìŠ¤íŠ¸ë¦¬ë°)
            response = openai.chat.completions.create(
                model="gpt-4-turbo-preview",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=2500,
                temperature=0.1,  # ì •í™•ì„±ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„
                stream=False
            )
            
            full_response = response.choices[0].message.content
            
            # ì‘ë‹µ ê²€ì¦ - ê¸°ì‚¬ ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
            verified_response = self._verify_ai_response(full_response, verified_articles, question)
            
            # ì‘ë‹µ íŒŒì‹± ë° ê°ì£¼ ê²€ì¦
            return self._parse_and_validate_ai_response(verified_response, top_references, related_keywords)
            
        except Exception as e:
            self.logger.error(f"AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë ¨ ê¸°ì‚¬ëŠ” {len(verified_articles)}ê°œë¥¼ ì°¾ì•˜ìœ¼ë‚˜ ë¶„ì„ì„ ì™„ë£Œí•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.",
                "summary": "AI ë¶„ì„ ì‹¤íŒ¨",
                "key_points": ["AI ë¶„ì„ì„ ì™„ë£Œí•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.", "ê´€ë ¨ ê¸°ì‚¬ëŠ” ê²€ìƒ‰ë˜ì—ˆìœ¼ë‚˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."],
                "citations_used": [],
                "related_keywords": related_keywords
            }
    
    def _verify_article_relevance(self, articles: List[Dict[str, Any]], question: str, keywords: List[str]) -> List[Dict[str, Any]]:
        """ê¸°ì‚¬ì™€ ì§ˆë¬¸ì˜ ê´€ë ¨ì„±ì„ ì—„ê²©í•˜ê²Œ ê²€ì¦"""
        
        # ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        question_lower = question.lower()
        question_keywords = []
        
        # ê¸°ìˆ  ìš©ì–´ ìš°ì„  ì¶”ì¶œ
        tech_terms = ['hbm', 'ai', 'iot', 'esg', 'cpu', 'gpu', 'dram', 'ssd', 'ë°˜ë„ì²´', 'ë©”ëª¨ë¦¬', 'ì¸ê³µì§€ëŠ¥']
        for term in tech_terms:
            if term in question_lower:
                question_keywords.append(term)
        
        # ì¼ë°˜ í‚¤ì›Œë“œ ì¶”ê°€
        question_keywords.extend([kw.lower() for kw in keywords[:3]])
        
        verified_articles = []
        
        for article in articles:
            title = article.get("title", "").lower()
            content = article.get("content", "").lower()
            full_text = f"{title} {content}"
            
            # í•µì‹¬ í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
            matches = 0
            for keyword in question_keywords:
                if keyword in full_text:
                    matches += 1
            
            # 50% ì´ìƒ í‚¤ì›Œë“œê°€ ë§¤ì¹­ë˜ë©´ ê´€ë ¨ ê¸°ì‚¬ë¡œ ì¸ì •
            if matches >= len(question_keywords) * 0.5:
                verified_articles.append(article)
                
        self.logger.info(f"ê´€ë ¨ì„± ê²€ì¦: {len(verified_articles)}/{len(articles)} ê¸°ì‚¬ê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ë¨")
        return verified_articles[:10]  # ìµœëŒ€ 10ê°œ
    
    def _verify_ai_response(self, response: str, articles: List[Dict[str, Any]], question: str) -> str:
        """AI ì‘ë‹µì´ ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš©ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì¦"""
        
        # ê¸°ì‚¬ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        all_article_text = ""
        for article in articles:
            title = article.get("title", "")
            content = article.get("content", "")
            highlight = article.get("highlight", {})
            
            all_article_text += f"{title} {content} "
            
            # í•˜ì´ë¼ì´íŠ¸ ì •ë³´ë„ í¬í•¨
            if highlight:
                if "title" in highlight:
                    all_article_text += " ".join(highlight["title"]) + " "
                if "content" in highlight:
                    all_article_text += " ".join(highlight["content"]) + " "
        
        all_article_text = all_article_text.lower()
        
        # ì‘ë‹µì—ì„œ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë‚´ìš© ê²€ì¶œ
        response_lines = response.split('\n')
        verified_lines = []
        
        for line in response_lines:
            if not line.strip():
                verified_lines.append(line)
                continue
                
            line_lower = line.lower()
            
            # ì¼ë°˜ì ì¸ ì§€ì‹ì´ë‚˜ ì¶”ì¸¡ì„± í‘œí˜„ ê²€ì¶œ
            suspicious_phrases = [
                'ì¼ë°˜ì ìœ¼ë¡œ', 'ë³´í†µ', 'ëŒ€ì²´ë¡œ', 'ì˜ˆìƒë©ë‹ˆë‹¤', 'ì¶”ì •ë©ë‹ˆë‹¤', 
                'ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤', 'ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤', 'ì „ë§ì…ë‹ˆë‹¤'
            ]
            
            is_suspicious = any(phrase in line_lower for phrase in suspicious_phrases)
            
            if is_suspicious:
                # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë‚´ìš©ì€ "ê¸°ì‚¬ì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŒ" í‘œì‹œ ì¶”ê°€
                verified_lines.append(line + " (ê¸°ì‚¬ì—ì„œ í™•ì¸ë˜ì§€ ì•Šì€ ë‚´ìš©)")
            else:
                verified_lines.append(line)
        
        return '\n'.join(verified_lines)
    
    def _parse_and_validate_ai_response(self, response_text: str, references: List[ArticleReference], related_keywords: List[str]) -> Dict[str, Any]:
        """AI ì‘ë‹µ íŒŒì‹± ë° ê°ì£¼ ê²€ì¦ (ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”)"""
        
        try:
            # ì „ì²´ ì‘ë‹µì„ answerë¡œ ì‚¬ìš©
            answer = response_text.strip()
            
            # ì‘ë‹µì´ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            if not answer:
                self.logger.warning("AI ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return self._create_fallback_response(response_text, references, related_keywords, "AI ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            
            # ë””ë²„ê¹…: ì›ë³¸ ì‘ë‹µ í™•ì¸
            self.logger.info(f"AI ì›ë³¸ ì‘ë‹µ (ì²˜ìŒ 200ì): {answer[:200]}...")
            
            # ê¸°ì¡´ ì¸ìš© ë²ˆí˜¸ íŒ¨í„´ í™•ì¸
            existing_citation_patterns = [
                r'\d+(?=\s*[.!?]?\s*$)',  # ë¬¸ì¥ ë ìˆ«ì
                r'([ê°€-í£a-zA-Z])(\d+)(?=\s|$|[.!?])',  # í•œê¸€/ì˜ë¬¸ ë’¤ ìˆ«ì
                r'([.!?])(\d+)(?=\s|$)',  # ë¬¸ì¥ë¶€í˜¸ ë’¤ ìˆ«ì
                r'(\S)(\d+)(?=\s|$)'  # ë¹„ê³µë°± ë¬¸ì ë’¤ ìˆ«ì
            ]
            
            has_existing_citations = False
            for pattern in existing_citation_patterns:
                if re.search(pattern, answer):
                    has_existing_citations = True
                    break
            
            # ì¸ìš© ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ ì•ˆì „í•˜ê²Œ ì¶”ê°€
            if not has_existing_citations:
                self.logger.info("AI ì‘ë‹µì— ì¸ìš© ë²ˆí˜¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì•ˆì „í•˜ê²Œ ì¸ìš© ë²ˆí˜¸ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")
                
                try:
                    # ë¬¸ì¥ ë¶„ë¦¬ (í•œêµ­ì–´ ë¬¸ì¥ë¶€í˜¸ ê¸°ì¤€) - ë” ì•ˆì „í•œ ë°©ë²•
                    sentences = re.split(r'(?<=[.!?])\s+', answer)
                    new_sentences = []
                    
                    for i, sentence in enumerate(sentences):
                        sentence = sentence.strip()
                        if sentence and len(sentence) > 5:  # ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ë§Œ ì²˜ë¦¬
                            # ì¸ìš©ë²ˆí˜¸ ì¶”ê°€ (1-10 ìˆœí™˜, references ê¸¸ì´ ê³ ë ¤)
                            max_ref = min(len(references), 10) if references else 3
                            citation_num = (i % max_ref) + 1
                            
                            # ë¬¸ì¥ ëì— ë¬¸ì¥ë¶€í˜¸ê°€ ìˆëŠ”ì§€ í™•ì¸
                            if sentence.endswith(('.', '!', '?')):
                                # ë¬¸ì¥ë¶€í˜¸ ì „ì— ì¸ìš©ë²ˆí˜¸ ì‚½ì…
                                new_sentence = sentence[:-1] + str(citation_num) + sentence[-1]
                            else:
                                # ë¬¸ì¥ë¶€í˜¸ê°€ ì—†ìœ¼ë©´ ë§ˆì¹¨í‘œì™€ í•¨ê»˜ ì¶”ê°€
                                new_sentence = sentence + str(citation_num) + "."
                            
                            new_sentences.append(new_sentence)
                        elif sentence:  # ì§§ì€ ë¬¸ì¥ë„ ë³´ì¡´
                            new_sentences.append(sentence)
                    
                    if new_sentences:
                        answer = " ".join(new_sentences)
                        self.logger.info("ì¸ìš© ë²ˆí˜¸ ì¶”ê°€ ì™„ë£Œ")
                    
                except Exception as citation_error:
                    self.logger.warning(f"ì¸ìš© ë²ˆí˜¸ ì¶”ê°€ ì‹¤íŒ¨: {citation_error}, ì›ë³¸ ì‘ë‹µ ì‚¬ìš©")
                    # ì¸ìš© ë²ˆí˜¸ ì¶”ê°€ì— ì‹¤íŒ¨í•´ë„ ì›ë³¸ ì‘ë‹µì€ ìœ ì§€
            
            # ì¸ìš© ë²ˆí˜¸ ì¶”ì¶œ - ì•ˆì „í•œ ì²˜ë¦¬
            citation_numbers = []
            try:
                citation_patterns = [
                    r'([ê°€-í£a-zA-Z.!?])(\d+)(?=\s|$|[.!?])',  # í•œê¸€/ì˜ë¬¸/ë¬¸ì¥ë¶€í˜¸ ë’¤ ìˆ«ì
                    r'(\w)(\d+)(?=\s|$)',  # ë‹¨ì–´ ë¬¸ì ë’¤ ìˆ«ì
                    r'(\S)(\d+)(?=\s|$|[.!?])'  # ë¹„ê³µë°± ë¬¸ì ë’¤ ìˆ«ì
                ]
                
                for pattern in citation_patterns:
                    matches = re.finditer(pattern, answer)
                    for match in matches:
                        try:
                            num_str = match.group(2)
                            if num_str and num_str.isdigit():
                                num = int(num_str)
                                max_citations = len(references) if references else 10
                                if 1 <= num <= max_citations:  # ì‹¤ì œ ì°¸ì¡° ë²”ìœ„ ë‚´ì—ì„œë§Œ
                                    citation_numbers.append(str(num))
                        except (IndexError, ValueError, AttributeError) as e:
                            self.logger.warning(f"ì¸ìš© ë²ˆí˜¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë¬´ì‹œ: {e}")
                            continue
                
                # ì¤‘ë³µ ì œê±°í•˜ë˜ ìˆœì„œ ìœ ì§€
                seen = set()
                citation_numbers = [x for x in citation_numbers if not (x in seen or seen.add(x))]
                
            except Exception as extraction_error:
                self.logger.warning(f"ì¸ìš© ë²ˆí˜¸ ì¶”ì¶œ ì‹¤íŒ¨: {extraction_error}")
                citation_numbers = []
            
            # ì¸ìš©ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ ì•ˆì „í•œ ê¸°ë³¸ê°’ ì„¤ì •
            if not citation_numbers and references:
                citation_numbers = ['1']  # ìµœì†Œí•œ í•˜ë‚˜ëŠ” ì„¤ì •
                self.logger.info("ê¸°ë³¸ ì¸ìš©ë²ˆí˜¸ 1 ì„¤ì •")
            
            # ì‚¬ìš©ëœ ì¸ìš© ì •ë³´ ìˆ˜ì§‘ - ë§¤ìš° ì•ˆì „í•œ ì²˜ë¦¬
            citations_used = []
            for citation_num in citation_numbers:
                try:
                    if not citation_num or not citation_num.isdigit():
                        continue
                        
                    ref_index = int(citation_num) - 1
                    
                    if references and 0 <= ref_index < len(references):
                        ref = references[ref_index]
                        if ref and hasattr(ref, 'title'):  # ì•ˆì „ì„± ê²€ì‚¬
                            citations_used.append({
                                "citation_number": int(citation_num),
                                "title": getattr(ref, 'title', 'ì œëª© ì—†ìŒ'),
                                "provider": getattr(ref, 'provider', 'Unknown'),
                                "published_at": getattr(ref, 'published_at', ''),
                                "relevance_score": getattr(ref, 'relevance_score', 0.5)
                            })
                            self.logger.debug(f"ì¶”ê°€ëœ ì¸ìš©: {citation_num}")
                        else:
                            self.logger.warning(f"ì°¸ì¡° ê°ì²´ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {ref_index}")
                    else:
                        self.logger.warning(f"ì¸ìš© ë²ˆí˜¸ {citation_num}ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨ (ì°¸ì¡° ê¸°ì‚¬ ìˆ˜: {len(references) if references else 0})")
                        
                except (ValueError, IndexError, AttributeError, TypeError) as e:
                    self.logger.warning(f"ì¸ìš© ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë¬´ì‹œ: {citation_num}, ì˜¤ë¥˜: {e}")
                    continue
            
            # ì¸ìš©ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ê¸°ì‚¬ë¼ë„ ì•ˆì „í•˜ê²Œ ì¶”ê°€
            if not citations_used and references and len(references) > 0:
                try:
                    first_ref = references[0]
                    if first_ref and hasattr(first_ref, 'title'):
                        citations_used.append({
                            "citation_number": 1,
                            "title": getattr(first_ref, 'title', 'ê¸°ë³¸ ì°¸ì¡°'),
                            "provider": getattr(first_ref, 'provider', 'Unknown'),
                            "published_at": getattr(first_ref, 'published_at', ''),
                            "relevance_score": getattr(first_ref, 'relevance_score', 0.5)
                        })
                        self.logger.info("ì²« ë²ˆì§¸ ê¸°ì‚¬ë¥¼ ê¸°ë³¸ ì¸ìš©ìœ¼ë¡œ ì¶”ê°€")
                except Exception as first_ref_error:
                    self.logger.warning(f"ì²« ë²ˆì§¸ ì°¸ì¡° ì¶”ê°€ ì‹¤íŒ¨: {first_ref_error}")
            
            # ìš”ì•½ ìƒì„± - ì•ˆì „í•œ ì²˜ë¦¬
            try:
                clean_text_for_summary = re.sub(r'(\S)(\d+)(?=\s|$|[.!?])', r'\1', answer)
                summary = clean_text_for_summary[:200] + "..." if len(clean_text_for_summary) > 200 else clean_text_for_summary
            except Exception as summary_error:
                self.logger.warning(f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {summary_error}")
                summary = answer[:200] + "..." if len(answer) > 200 else answer
            
            # ì£¼ìš” í¬ì¸íŠ¸ ìƒì„± - ì•ˆì „í•œ ì²˜ë¦¬
            key_points = []
            try:
                sentences = re.split(r'[.!?]\s+', answer)
                
                for sentence in sentences:
                    sentence = sentence.strip()
                    if len(sentence) > 30 and len(key_points) < 4:
                        # ì¸ìš© ë²ˆí˜¸ ì œê±°
                        clean_sentence = re.sub(r'(\S)(\d+)(?=\s|$)', r'\1', sentence)
                        clean_sentence = clean_sentence.strip()
                        if len(clean_sentence) > 30:
                            key_points.append(clean_sentence)
                
            except Exception as key_points_error:
                self.logger.warning(f"ì£¼ìš” í¬ì¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {key_points_error}")
            
            # ê¸°ë³¸ê°’ ì„¤ì •
            if not key_points:
                key_points = ["ë‰´ìŠ¤ ë¶„ì„ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”."]
            
            self.logger.info(f"ìµœì¢… ì²˜ë¦¬ ì™„ë£Œ - ì¸ìš©: {len(citations_used)}, í‚¤í¬ì¸íŠ¸: {len(key_points)}")
            
            return {
                "answer": answer,
                "summary": summary,
                "key_points": key_points,
                "citations_used": citations_used,
                "related_keywords": related_keywords or [],
                "total_citations": len(citation_numbers),
                "articles_used_count": len(citations_used)
            }
            
        except Exception as e:
            self.logger.error(f"AI ì‘ë‹µ íŒŒì‹± ì¤‘ ì „ì²´ ì˜¤ë¥˜: {e}", exc_info=True)
            # ì „ì²´ ì‹¤íŒ¨ ì‹œì—ë„ ì•ˆì „í•œ ì‘ë‹µ ë°˜í™˜
            return self._create_fallback_response(response_text, references, related_keywords, str(e))
    
    def _create_fallback_response(self, response_text: str, references: List[ArticleReference], related_keywords: List[str], error_msg: str) -> Dict[str, Any]:
        """íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì•ˆì „í•œ ëŒ€ì²´ ì‘ë‹µ ìƒì„±"""
        
        # ì‘ë‹µ í…ìŠ¤íŠ¸ ì •ë¦¬
        clean_response = response_text.strip() if response_text else "ë¶„ì„ ê²°ê³¼ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        # ê¸°ë³¸ ìš”ì•½
        summary = clean_response[:150] + "..." if len(clean_response) > 150 else clean_response
        
        # ê¸°ë³¸ í‚¤í¬ì¸íŠ¸
        key_points = [
            "ê¸°ì‚¬ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "ìì„¸í•œ ë‚´ìš©ì€ ì°¸ì¡° ê¸°ì‚¬ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        ]
        
        # ì•ˆì „í•œ ì²« ë²ˆì§¸ ì¸ìš© ì¶”ê°€
        citations_used = []
        if references and len(references) > 0:
            try:
                first_ref = references[0]
                citations_used = [{
                    "citation_number": 1,
                    "title": getattr(first_ref, 'title', 'ì°¸ì¡° ê¸°ì‚¬'),
                    "provider": getattr(first_ref, 'provider', 'Unknown'),
                    "published_at": getattr(first_ref, 'published_at', ''),
                    "relevance_score": getattr(first_ref, 'relevance_score', 0.5)
                }]
            except Exception as fallback_error:
                self.logger.warning(f"ëŒ€ì²´ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {fallback_error}")
        
        self.logger.info(f"ì•ˆì „í•œ ëŒ€ì²´ ì‘ë‹µ ìƒì„± ì™„ë£Œ: {error_msg}")
        
        return {
            "answer": clean_response,
            "summary": summary,
            "key_points": key_points,
            "citations_used": citations_used,
            "related_keywords": related_keywords or [],
            "total_citations": len(citations_used),
            "articles_used_count": len(citations_used)
        }
    
    async def generate_concierge_response_stream_with_ai_streaming(
        self, 
        request: ConciergeRequest
    ) -> AsyncGenerator[ConciergeProgress, None]:
        """
        ì‹¤ì‹œê°„ GPT-4 ìŠ¤íŠ¸ë¦¬ë°ì„ í¬í•¨í•œ AI ë‰´ìŠ¤ ì»¨ì‹œì–´ì§€ ì‘ë‹µ ìƒì„±
        
        Args:
            request: ì»¨ì‹œì–´ì§€ ìš”ì²­
            
        Yields:
            ConciergeProgress: ì§„í–‰ ìƒí™© (AI ì‘ë‹µ ì‹¤ì‹œê°„ í¬í•¨)
        """
        start_time = time.time()
        
        try:
            # 1-4ë‹¨ê³„: ê¸°ì¡´ê³¼ ë™ì¼ (ì§ˆë¬¸ ë¶„ì„, ê²€ìƒ‰, ì—°ê´€ì–´ ìˆ˜ì§‘)
            yield ConciergeProgress(
                stage="question_analysis",
                progress=5,
                message="ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                current_task="í‚¤ì›Œë“œ ì¶”ì¶œ"
            )
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            processed_query = self.query_processor.preprocess_query(request.question)
            extracted_keywords = [keyword for keyword, weight in processed_query]
            
            yield ConciergeProgress(
                stage="keywords_extracted",
                progress=15,
                message=f"í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {', '.join(extracted_keywords[:5])}",
                current_task="ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½",
                extracted_keywords=extracted_keywords
            )
            
            # ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½
            yield ConciergeProgress(
                stage="search_strategy",
                progress=25,
                message="ìµœì ì˜ ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                current_task="AND/OR ê²€ìƒ‰ ì „ëµ"
            )
            
            # ë‚ ì§œ ë²”ìœ„ ì„¤ì • - ìµœì‹ ì„± ê°•í™” (7ì¼ ìš°ì„ , 30ì¼ í´ë°±)
            date_from = request.date_from or (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")  # ìµœê·¼ 7ì¼
            date_to = request.date_to or (datetime.now() + timedelta(days=1)).strftime("%Y-%m-%d")
            
            # ë‰´ìŠ¤ ê²€ìƒ‰
            yield ConciergeProgress(
                stage="news_search",
                progress=45,
                message="ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                current_task="BigKinds API í˜¸ì¶œ"
            )
            
            search_results = await self._execute_advanced_search(
                request.question, date_from, date_to, 10  # 10ê°œë¡œ í™•ì¥
            )
            
            articles = search_results.get("documents", [])
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ì¡°ê¸° ì¢…ë£Œ
            if not articles or len(articles) == 0 or search_results.get("search_failed", False):
                error_message = search_results.get("error_message", f"'{request.question}'ì— ëŒ€í•œ ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                yield ConciergeProgress(
                    stage="no_results",
                    progress=100,
                    message=error_message,
                    current_task="ê²€ìƒ‰ ì™„ë£Œ",
                    search_results_count=0
                )
                
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œì˜ ì‘ë‹µ ìƒì„±
                search_strategy = {
                    "keywords": extracted_keywords,
                    "date_range": f"{date_from} ~ {date_to}",
                    "search_type": "AND_priority",
                    "max_articles": request.max_articles,
                    "include_related_keywords": request.include_related_keywords,
                    "include_today_issues": request.include_today_issues
                }
                
                # í‚¤ì›Œë“œ ê¸°ë°˜ ì—°ê´€ì–´ ìƒì„± (ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ë„ ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ” ì—°ê´€ì–´)
                related_keywords = []
                if extracted_keywords:
                    main_keyword = extracted_keywords[0]
                    related_keywords = self._generate_default_related_keywords(main_keyword)
                
                final_response = ConciergeResponse(
                    question=request.question,
                    answer=f"ì£„ì†¡í•©ë‹ˆë‹¤. '{request.question}'ì— ëŒ€í•œ ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\në‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ì„ ì‹œë„í•´ë³´ì„¸ìš”:\nâ€¢ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”\nâ€¢ ê²€ìƒ‰ ê¸°ê°„ì„ ì¡°ì •í•´ë³´ì„¸ìš”\nâ€¢ ë” ì¼ë°˜ì ì¸ ìš©ì–´ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”",
                    summary=f"'{request.question}' ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    key_points=[
                        "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.",
                        "ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”.",
                        "ê²€ìƒ‰ ê¸°ê°„ì„ ì¡°ì •í•´ë³´ì„¸ìš”.",
                        "ë” ì¼ë°˜ì ì¸ ìš©ì–´ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”."
                    ],
                    references=[],
                    related_keywords=related_keywords,
                    today_issues=[],
                    search_strategy=search_strategy,
                    analysis_metadata={
                        "processing_time_seconds": round(time.time() - start_time, 2),
                        "articles_analyzed": 0,
                        "keywords_extracted": len(extracted_keywords),
                        "ai_model": "none",
                        "generated_at": datetime.now().isoformat(),
                        "error": "no_search_results",
                        "search_attempted": True
                    },
                    generated_at=datetime.now().isoformat()
                )
                
                yield ConciergeProgress(
                    stage="completed",
                    progress=100,
                    message="ê²€ìƒ‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                    current_task="ì™„ë£Œ",
                    result=final_response
                )
                return
            
            yield ConciergeProgress(
                stage="search_completed",
                progress=55,
                message=f"{len(articles)}ê°œì˜ ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.",
                current_task="ì—°ê´€ì–´ ë° ì´ìŠˆ ìˆ˜ì§‘",
                search_results_count=len(articles)
            )
            
            # ì—°ê´€ì–´ ë° ì˜¤ëŠ˜ì˜ ì´ìŠˆ ë³‘ë ¬ ìˆ˜ì§‘ (ì„±ëŠ¥ ìµœì í™”)
            related_keywords = []
            today_issues = []
            
            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ íƒœìŠ¤í¬ ë¦¬ìŠ¤íŠ¸
            tasks = []
            
            if request.include_related_keywords and extracted_keywords:
                yield ConciergeProgress(
                    stage="related_keywords",
                    progress=65,
                    message="ì—°ê´€ í‚¤ì›Œë“œë¥¼ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                    current_task="ì—°ê´€ì–´ API"
                )
                main_keyword = extracted_keywords[0] if extracted_keywords else request.question
                tasks.append(self._get_related_keywords(main_keyword))
            else:
                tasks.append(asyncio.create_task(asyncio.sleep(0)))  # ë”ë¯¸ íƒœìŠ¤í¬
            
            if request.include_today_issues:
                yield ConciergeProgress(
                    stage="today_issues",
                    progress=70,
                    message="ì˜¤ëŠ˜ì˜ ì´ìŠˆë¥¼ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                    current_task="ì´ìŠˆ ë­í‚¹"
                )
                tasks.append(self._get_today_issues())
            else:
                tasks.append(asyncio.create_task(asyncio.sleep(0)))  # ë”ë¯¸ íƒœìŠ¤í¬
            
            # ë³‘ë ¬ ì‹¤í–‰
            if tasks and len(tasks) >= 2:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # ì—°ê´€ì–´ ê²°ê³¼ ì²˜ë¦¬
                if request.include_related_keywords and extracted_keywords and len(results) > 0:
                    if not isinstance(results[0], Exception) and isinstance(results[0], list):
                        related_keywords = results[0]
                    else:
                        self.logger.warning(f"ì—°ê´€ì–´ ìˆ˜ì§‘ ì‹¤íŒ¨: {results[0] if len(results) > 0 else 'No results'}")
                        related_keywords = []
                
                # ì˜¤ëŠ˜ì˜ ì´ìŠˆ ê²°ê³¼ ì²˜ë¦¬
                if request.include_today_issues and len(results) > 1:
                    if not isinstance(results[1], Exception) and isinstance(results[1], list):
                        today_issues = results[1]
                    else:
                        self.logger.warning(f"ì˜¤ëŠ˜ì˜ ì´ìŠˆ ìˆ˜ì§‘ ì‹¤íŒ¨: {results[1] if len(results) > 1 else 'No results'}")
                        today_issues = []
            
            # ê´€ë ¨ ì§ˆë¬¸ ìƒì„± (ì—°ê´€ì–´ ê¸°ë°˜)
            related_questions = []
            if request.include_related_questions and related_keywords:
                yield ConciergeProgress(
                    stage="related_questions",
                    progress=72,
                    message="ì—°ê´€ì–´ ê¸°ë°˜ ê´€ë ¨ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                    current_task="ê´€ë ¨ ì§ˆë¬¸ ìƒì„±"
                )
                
                # í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ìƒì„± (ì¶”ì¶œëœ í‚¤ì›Œë“œ ìˆœì„œ ê¸°ë°˜)
                keyword_weights = {}
                for i, keyword in enumerate(extracted_keywords):
                    if keyword in related_keywords:
                        keyword_weights[keyword] = 1.0 - (i * 0.1)  # ìˆœì„œëŒ€ë¡œ ê°€ì¤‘ì¹˜ ê°ì†Œ
                
                # ì—°ê´€ì–´ì—ë„ ê°€ì¤‘ì¹˜ ë¶€ì—¬
                for i, keyword in enumerate(related_keywords):
                    if keyword not in keyword_weights:
                        keyword_weights[keyword] = 0.8 - (i * 0.05)  # ì—°ê´€ì–´ëŠ” ë‚®ì€ ê°€ì¤‘ì¹˜
                
                # ê´€ë ¨ ì§ˆë¬¸ ìƒì„±
                related_questions = self.questions_generator.generate_related_questions(
                    original_question=request.question,
                    related_keywords=related_keywords[:8],  # ìƒìœ„ 8ê°œ ì—°ê´€ì–´ë§Œ ì‚¬ìš©
                    keyword_weights=keyword_weights,
                    max_questions=6
                )
                
                self.logger.info(f"ê´€ë ¨ ì§ˆë¬¸ ìƒì„± ì™„ë£Œ: {len(related_questions)}ê°œ")
            
            # 5ë‹¨ê³„: AI ë¶„ì„ ë° ë‹µë³€ ìƒì„±
            yield ConciergeProgress(
                stage="ai_analysis",
                progress=75,
                message="AIê°€ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ê³  ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                current_task="GPT-4 ë¶„ì„"
            )
            
            # ê¸°ì‚¬ ì°¸ì¡° ì •ë³´ ìƒì„±
            references = self._create_article_references(articles)
            
            # AI ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            streaming_response = ""
            async for chunk in self._generate_ai_streaming_response(
                request.question, articles, references, related_keywords, 
                today_issues, request.detail_level
            ):
                streaming_response += chunk
                
                # ìŠ¤íŠ¸ë¦¬ë° ì§„í–‰ìƒí™© ì „ì†¡ (ì°¸ì¡° ì •ë³´ í¬í•¨)
                yield ConciergeProgress(
                    stage="ai_streaming",
                    progress=min(75 + int(len(streaming_response) / 10), 90),
                    message="AIê°€ ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                    current_task="ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ìƒì„±",
                    streaming_content=streaming_response,
                    result=ConciergeResponse(
                        question=request.question,
                        answer="",  # ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì´ë¯€ë¡œ ë¹ˆ ë¬¸ìì—´
                        summary="",
                        key_points=[],
                        references=references,  # ì°¸ì¡° ì •ë³´ë§Œ ë¯¸ë¦¬ ì œê³µ
                        related_keywords=[],
                        today_issues=[],
                        search_strategy={},
                        analysis_metadata={},
                        generated_at=datetime.now().isoformat()
                    ) if references else None
                )
            
            # ìµœì¢… ì‘ë‹µ íŒŒì‹±
            parsed_response = self._parse_and_validate_ai_response(streaming_response, references, related_keywords)
            
            # ê²€ìƒ‰ ì „ëµ êµ¬ì„±
            search_strategy = {
                "keywords": extracted_keywords,
                "date_range": f"{date_from} ~ {date_to}",
                "search_type": "AND_priority",
                "max_articles": request.max_articles,
                "include_related_keywords": request.include_related_keywords,
                "include_today_issues": request.include_today_issues
            }
            
            # ìµœì¢… ì‘ë‹µ ìƒì„±
            final_response = ConciergeResponse(
                question=request.question,
                answer=parsed_response["answer"],
                summary=parsed_response["summary"],
                key_points=parsed_response["key_points"],
                references=references,
                related_keywords=related_keywords,
                related_questions=related_questions,
                today_issues=today_issues,
                search_strategy=search_strategy,
                analysis_metadata={
                    "processing_time_seconds": round(time.time() - start_time, 2),
                    "articles_analyzed": len(articles),
                    "keywords_extracted": len(extracted_keywords),
                    "ai_model": "gpt-4o-mini",  # ì‹¤ì œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ë¡œ ìˆ˜ì •
                    "generated_at": datetime.now().isoformat(),
                    "citations_used": parsed_response.get("citations_used", []),
                    "total_citations": parsed_response.get("total_citations", 0),
                    "related_keywords": related_keywords,
                    "related_questions_count": len(related_questions)
                },
                generated_at=datetime.now().isoformat()
            )
            
            # ì™„ë£Œ
            yield ConciergeProgress(
                stage="completed",
                progress=100,
                message="AI ë‰´ìŠ¤ ì»¨ì‹œì–´ì§€ ë‹µë³€ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!",
                current_task="ì™„ë£Œ",
                result=final_response
            )
            
        except Exception as e:
            self.logger.error(f"ì»¨ì‹œì–´ì§€ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            yield ConciergeProgress(
                stage="error",
                progress=0,
                message=f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                current_task="ì˜¤ë¥˜ ì²˜ë¦¬"
            )

    async def _generate_ai_streaming_response(
        self,
        question: str,
        articles: List[Dict[str, Any]],
        references: List[ArticleReference],
        related_keywords: List[str],
        today_issues: List[Dict[str, Any]],
        detail_level: str
    ) -> AsyncGenerator[str, None]:
        """GPT-4 ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (ê°ì£¼ ì‹œìŠ¤í…œ í¬í•¨)"""
        
        # 10ê°œ ê¸°ì‚¬ ì‚¬ìš©
        top_articles = articles[:10]
        
        # ê¸°ì‚¬ ë‚´ìš© êµ¬ì„± (ê¸°ì¡´ê³¼ ë™ì¼í•˜ì§€ë§Œ ë” ìƒì„¸í•˜ê²Œ)
        articles_text = ""
        for i, article in enumerate(top_articles):
            ref_id = f"ref{i+1}"
            title = article.get("title", "")
            content = article.get("content", article.get("summary", ""))[:800]
            provider = article.get("provider", "")
            published_at = article.get("published_at", "")
            
            articles_text += f"\n[{ref_id}] ì œëª©: {title}\n"
            articles_text += f"ì–¸ë¡ ì‚¬: {provider} | ë°œí–‰ì¼: {published_at}\n"
            articles_text += f"ë‚´ìš©: {content}\n"
            articles_text += "=" * 50 + "\n"
        
        # ì—°ê´€ í‚¤ì›Œë“œ í…ìŠ¤íŠ¸
        related_text = ""
        if related_keywords:
            related_text = f"\nì£¼ìš” ì—°ê´€ í‚¤ì›Œë“œ: {', '.join(related_keywords[:10])}\n"
        
        # ì˜¤ëŠ˜ì˜ ì´ìŠˆ í…ìŠ¤íŠ¸
        issues_text = ""
        if today_issues:
            issues_text = "\nê´€ë ¨ ì˜¤ëŠ˜ì˜ ì£¼ìš” ì´ìŠˆ:\n"
            for issue in today_issues[:3]:
                issues_text += f"- {issue.get('title', issue.get('keyword', ''))}\n"
        
        # ìƒì„¸ë„ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì¡°ì • (detailedë¡œ ê³ ì •ë˜ì—ˆìœ¼ë¯€ë¡œ ì¤‘ê°„ ìˆ˜ì¤€)
        response_instruction = "ìƒì„¸í•˜ê³  êµ¬ì²´ì ì¸ ë¶„ì„ ë‹µë³€ (800-1000ì)"
        
        # GPT-4 í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ê°ì£¼ ì‹œìŠ¤í…œ ê°•í™” - ì¤‘ì•™ì¼ë³´ ìŠ¤íƒ€ì¼)
        system_prompt = """ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê°ê´€ì ì´ê³  í†µì°°ë ¥ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

â˜…â˜…â˜… í•µì‹¬ ê·œì¹™: ëª¨ë“  ë¬¸ì¥ì€ ë°˜ë“œì‹œ ì¸ìš© ë²ˆí˜¸(1~10)ë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤ â˜…â˜…â˜…

ë‹µë³€ ì‘ì„± ê·œì¹™:
1. ë°˜ë“œì‹œ ì œê³µëœ ê¸°ì‚¬ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. **ëª¨ë“  ë¬¸ì¥ì˜ ëì— ì¸ìš© ë²ˆí˜¸ë¥¼ í‘œì‹œí•˜ì„¸ìš”** (ì˜ˆ: ë¬¸ì¥ ëì— 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
3. ì¸ìš© ë²ˆí˜¸ëŠ” ë¬¸ì¥ë¶€í˜¸ ë°”ë¡œ ë’¤ì— ê³µë°± ì—†ì´ ìˆ«ìë§Œ í‘œì‹œ
4. ì˜¬ë°”ë¥¸ ì˜ˆ: "ë°œí‘œí–ˆë‹¤1", "ì¦ê°€í–ˆë‹¤2", "ì˜ˆì •ì´ë‹¤3", "ë¶„ì„í–ˆë‹¤8", "ì „ë§ì´ë‹¤10"
5. ì˜ëª»ëœ ì˜ˆ: "ë°œí‘œí–ˆë‹¤ 1", "ë°œí‘œí–ˆë‹¤[1]", "ë°œí‘œí–ˆë‹¤(1)"
6. ì¶”ì¸¡ì´ë‚˜ ê°œì¸ì  ì˜ê²¬ë³´ë‹¤ëŠ” ê¸°ì‚¬ì— ë‚˜íƒ€ë‚œ ì‚¬ì‹¤ê³¼ ë°ì´í„°ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”
7. ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰¬ìš´ íë¦„ìœ¼ë¡œ ì‘ì„±í•˜ë˜, ì •ë³´ì˜ ì¶œì²˜ë¥¼ ëª…í™•íˆ í•´ì£¼ì„¸ìš”
8. ê¸°ì‚¬ì—ì„œ ì¸ìš©í•œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ë°œì–¸ì´ ìˆë‹¤ë©´ ë¬¸ì¥ ëì— í•´ë‹¹ ê¸°ì‚¬ ë²ˆí˜¸ë¥¼ í‘œì‹œí•˜ì„¸ìš”
9. í•œ ë¬¸ì¥ì— ì—¬ëŸ¬ ê¸°ì‚¬ì˜ ì •ë³´ê°€ ìˆìœ¼ë©´ ê°€ì¥ ì¤‘ìš”í•œ ì¶œì²˜ í•˜ë‚˜ë§Œ í‘œì‹œ

â˜…â˜…â˜… ê°€ë…ì„± í–¥ìƒ ê·œì¹™ â˜…â˜…â˜…:
- **ë…¼ë¦¬ì  êµ¬ì¡°**: í˜„ì¬ ìƒí™© â†’ ë°°ê²½/ì›ì¸ â†’ ì£¼ìš” ë³€í™” â†’ í–¥í›„ ì „ë§ ìˆœìœ¼ë¡œ êµ¬ì„±
- **ì†Œì œëª© í™œìš©**: ë‚´ìš©ì´ ê¸¸ì–´ì§ˆ ê²½ìš° **í˜„ì¬ ìƒí™©**, **ì£¼ìš” ë³€í™”**, **í–¥í›„ ì „ë§** ë“± ì†Œì œëª© ì‚¬ìš©
- **ë¬¸ì¥ ê¸¸ì´ ì¡°ì ˆ**: í•œ ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ì§€ ì•Šë„ë¡ ì ì ˆíˆ ë‚˜ëˆ„ì–´ ì‘ì„±

â‘¡ êµ¬ì²´ì  ì •ë³´ í¬í•¨ ì˜ë¬´ (ë§¤ìš° ì¤‘ìš”):
- **ì¸ëª…**: ê´€ë ¨ëœ ëª¨ë“  ì¸ë¬¼ì˜ ì‹¤ëª…ê³¼ ì§ì±…ì„ ì •í™•íˆ ëª…ì‹œ (ì˜ˆ: "í™ê¸¸ë™ ê¸ˆìœµìœ„ì›ì¥ì— ë”°ë¥´ë©´")
- **ì§€ëª…**: êµ¬ì²´ì ì¸ ì§€ì—­ëª…, êµ­ê°€ëª…, ë„ì‹œëª… ë“±ì„ ëª…í™•íˆ í‘œê¸°  
- **ë‚ ì§œ**: êµ¬ì²´ì ì¸ ë‚ ì§œ, ì‹œê°„, ê¸°ê°„, ì‹œì ì„ ì •í™•íˆ ê¸°ì¬
- **ê¸°ê´€ëª…**: ê´€ë ¨ ê¸°ê´€, íšŒì‚¬, ì¡°ì§ì˜ ì •í™•í•œ ëª…ì¹­ í¬í•¨
- **ìˆ˜ì¹˜**: ê¸ˆì•¡, ë¹„ìœ¨, ê·œëª¨ ë“± êµ¬ì²´ì  ìˆ˜ì¹˜ ë°˜ë“œì‹œ í¬í•¨
- **ì¶œì²˜ í‘œí˜„**: "~ì— ë”°ë¥´ë©´", "~ë¼ê³  ë°í˜”ë‹¤" ë“± ì›ë¬¸ í‘œí˜„ í™œìš©"""

        user_prompt = f"""ì§ˆë¬¸: {question}

{response_instruction}ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¶„ì„í•  ê¸°ì‚¬ë“¤ (ë°˜ë“œì‹œ ì´ ê¸°ì‚¬ë“¤ì˜ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”):
{articles_text}

{related_text}

{issues_text}

ìœ„ 10ê°œ ê¸°ì‚¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. 

â˜…â˜…â˜… ì¸ìš© ë²ˆí˜¸ í‘œì‹œ í•„ìˆ˜ ê·œì¹™ (ë§¤ìš° ì¤‘ìš”) â˜…â˜…â˜…:
1. **ëª¨ë“  ë¬¸ì¥ì€ ë°˜ë“œì‹œ ì¸ìš© ë²ˆí˜¸ë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤**
2. ì¸ìš© ë²ˆí˜¸ëŠ” ë¬¸ì¥ë¶€í˜¸(ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ) ë°”ë¡œ ë’¤ì— **ê³µë°± ì—†ì´** ìˆ«ìë§Œ í‘œì‹œ
3. ì˜¬ë°”ë¥¸ í˜•ì‹: "ë°œí‘œí–ˆë‹¤1", "ì¦ê°€í–ˆë‹¤2", "ì˜ˆì •ì´ë‹¤3"
4. ì˜ëª»ëœ í˜•ì‹: "ë°œí‘œí–ˆë‹¤ 1", "ë°œí‘œí–ˆë‹¤[1]", "ë°œí‘œí–ˆë‹¤(1)"
5. í•œ ë¬¸ì¥ì— ì—¬ëŸ¬ ê¸°ì‚¬ ì •ë³´ê°€ ìˆìœ¼ë©´ ì£¼ìš” ì¶œì²˜ í•˜ë‚˜ë§Œ í‘œì‹œ

â˜…â˜…â˜… ê°€ë…ì„± í–¥ìƒ í•„ìˆ˜ ê·œì¹™ â˜…â˜…â˜…:
1. **ë¬¸ë‹¨ êµ¬ë¶„**: ì£¼ì œê°€ ë°”ë€” ë•Œë§ˆë‹¤ ë°˜ë“œì‹œ ë¹ˆ ì¤„(\\n\\n)ë¡œ ë¬¸ë‹¨ì„ ë‚˜ëˆ„ì„¸ìš”
2. **ë…¼ë¦¬ì  êµ¬ì¡°**: ë‹¤ìŒ ìˆœì„œë¡œ ì‘ì„±í•˜ì„¸ìš”
   - **í˜„ì¬ ìƒí™©**: ìµœì‹  ë™í–¥ê³¼ í˜„ì¬ ìƒíƒœ
   - **ë°°ê²½ ì •ë³´**: ì´ì— ì´ë¥´ê²Œ ëœ ë°°ê²½ì´ë‚˜ ì›ì¸
   - **ì£¼ìš” ë³€í™”**: í•µì‹¬ì ì¸ ë³€í™”ë‚˜ ì‚¬ê±´ë“¤
   - **í–¥í›„ ì „ë§**: ë¯¸ë˜ ê³„íšì´ë‚˜ ì˜ˆìƒë˜ëŠ” ì˜í–¥
3. **ì†Œì œëª© ì‚¬ìš©**: ë‚´ìš©ì´ ê¸¸ì–´ì§ˆ ê²½ìš° **í˜„ì¬ ìƒí™©**, **ì£¼ìš” ë³€í™”**, **í–¥í›„ ì „ë§** ë“± êµµì€ ê¸€ì”¨ë¡œ ì†Œì œëª© í‘œì‹œ
4. **ì ì ˆí•œ ë¬¸ì¥ ê¸¸ì´**: í•œ ë¬¸ì¥ì´ 3ì¤„ì„ ë„˜ì§€ ì•Šë„ë¡ ì¡°ì ˆí•˜ì—¬ ì½ê¸° ì‰½ê²Œ ì‘ì„±

ì¸ìš© ë²ˆí˜¸ ì˜ˆì‹œ:
- "ì‚¼ì„±ì „ìëŠ” ì˜¬í•´ HBM ë§¤ì¶œì´ ì „ë…„ ëŒ€ë¹„ 50% ì¦ê°€í–ˆë‹¤ê³  ë°œí‘œí–ˆë‹¤1"

(ë¬¸ë‹¨ ë‚˜ëˆ„ê¸°)

- "AI ë°˜ë„ì²´ ìˆ˜ìš” ê¸‰ì¦ìœ¼ë¡œ í–¥í›„ ì „ë§ë„ ë°ì€ ê²ƒìœ¼ë¡œ ë¶„ì„ëœë‹¤2"

ì¶”ê°€ ì§€ì¹¨:
- ê° ë¬¸ì¥ì´ë‚˜ ì •ë³´ì˜ ëì— í•´ë‹¹ ê¸°ì‚¬ ë²ˆí˜¸ë¥¼ í‘œì‹œí•˜ì„¸ìš” (1, 2, 3, 4, 5)
- ê¸°ì‚¬ì— ë‚˜ì˜¨ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ë°œì–¸, ê³„íš ë“±ì„ ì¸ìš©í•  ë•ŒëŠ” í•´ë‹¹ ë¬¸ì¥ ëì— ê¸°ì‚¬ ë²ˆí˜¸ë¥¼ í‘œì‹œí•˜ì„¸ìš”
- ì—¬ëŸ¬ ê¸°ì‚¬ì—ì„œ ë¹„ìŠ·í•œ ë‚´ìš©ì´ ë‚˜ì˜¨ë‹¤ë©´ ì ì ˆí•œ ê¸°ì‚¬ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì—¬ í‘œì‹œí•˜ì„¸ìš”
- ê¸°ì‚¬ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”

êµ¬ì²´ì  ì •ë³´ í•„ìˆ˜ í¬í•¨ì‚¬í•­:
- ì¸ë¬¼ ì–¸ê¸‰ ì‹œ: "í™ê¸¸ë™ XXíšŒì‚¬ ëŒ€í‘œ", "ê¹€ì² ìˆ˜ ê¸ˆìœµìœ„ì›ì¥" ë“± ì‹¤ëª…+ì§ì±… ëª…ì‹œ
- ì‹œê°„ ì •ë³´: "7ì›” 5ì¼", "ì˜¤ì „ 9ì‹œ", "2024ë…„ ìƒë°˜ê¸°" ë“± êµ¬ì²´ì  ì‹œì  í‘œê¸°
- ì¥ì†Œ ì •ë³´: "ì„œìš¸ ê°•ë‚¨êµ¬", "ë¯¸êµ­ ë‰´ìš•", "ì¤‘êµ­ ë² ì´ì§•" ë“± êµ¬ì²´ì  ì§€ëª…
- ê¸°ê´€ëª…: "ì‚¼ì„±ì „ì", "ê¸ˆìœµìœ„ì›íšŒ", "í•œêµ­ì€í–‰" ë“± ì •í™•í•œ ê¸°ê´€ëª…
- ìˆ˜ì¹˜ ë°ì´í„°: "30% ì¦ê°€", "1ì¡°ì› ê·œëª¨", "500ë§Œ ë‹¬ëŸ¬" ë“± êµ¬ì²´ì  ìˆ˜ì¹˜
- ì¸ìš©êµ¬: "~ë¼ê³  ë§í–ˆë‹¤", "~ì— ë”°ë¥´ë©´", "~ë¡œ ì „í•´ì¡Œë‹¤" ë“± ì›ë¬¸ í‘œí˜„ í™œìš©"""

        try:
            # GPT-4 ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ (ì„±ëŠ¥ ìµœì í™”)
            response = openai.chat.completions.create(
                model="gpt-4o-mini",  # ë” ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=2000,  # í† í° ìˆ˜ ì¤„ì—¬ì„œ ì†ë„ í–¥ìƒ
                temperature=0.1,  # ì¼ê´€ì„± í–¥ìƒ
                stream=True,  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
                stream_options={"include_usage": True}  # ì‚¬ìš©ëŸ‰ ì •ë³´ í¬í•¨
            )
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
            for chunk in response:
                if chunk.choices[0].delta.content is not None:
                    content = chunk.choices[0].delta.content
                    yield content
                    # ì‹¤ì‹œê°„ ì „ì†¡ì„ ìœ„í•œ ì‘ì€ ì§€ì—°
                    await asyncio.sleep(0.01)
            
        except Exception as e:
            self.logger.error(f"AI ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            yield f"ì£„ì†¡í•©ë‹ˆë‹¤. AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _filter_relevant_documents(self, documents: List[Dict[str, Any]], keywords: List[str], question: str) -> List[Dict[str, Any]]:
        """ê´€ë ¨ì„± ê¸°ë°˜ ë¬¸ì„œ í•„í„°ë§ - ì •í™•ë„ ìš°ì„  (ê°œì„ )"""
        
        if not documents or not keywords:
            return documents
        
        filtered_docs = []
        
        for doc in documents:
            title = doc.get("title", "").lower()
            content = doc.get("content", "").lower()
            full_text = f"{title} {content}"
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸ - ë” ì—„ê²©í•œ ê¸°ì¤€
            keyword_matches = 0
            title_matches = 0
            
            for keyword in keywords[:3]:  # ìµœëŒ€ 3ê°œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì²´í¬
                keyword_lower = keyword.lower()
                if keyword_lower in full_text:
                    keyword_matches += 1
                    
                    # ì œëª©ì— í¬í•¨ëœ ê²½ìš° ê°€ì¤‘ì¹˜ ë¶€ì—¬
                    if keyword_lower in title:
                        title_matches += 1
            
            # ì •í™•ë„ ê¸°ì¤€: ëª¨ë“  í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°ë§Œ ì„ íƒ (100% ë§¤ì¹­)
            if keyword_matches >= len(keywords[:3]):
                # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°: ì œëª© ë§¤ì¹­ì„ ë†’ê²Œ í‰ê°€
                relevance_score = title_matches * 10 + keyword_matches * 2
                
                # BigKinds ìì²´ ì ìˆ˜ë„ ê³ ë ¤
                bigkinds_score = doc.get("_score", 0) / 100.0 if doc.get("_score") else 0
                final_score = relevance_score + bigkinds_score
                
                doc["_relevance_score"] = final_score
                filtered_docs.append(doc)
        
        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬ (ë†’ì€ ì ìˆ˜ ìš°ì„ )
        filtered_docs.sort(key=lambda x: x.get("_relevance_score", 0), reverse=True)
        
        # ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥
        exact_keywords_str = ", ".join([f"'{kw}'" for kw in keywords[:3]])
        self.logger.info(f"ì—„ê²©í•œ í•„í„°ë§: {exact_keywords_str} ëª¨ë‘ í¬í•¨ í•„ìˆ˜, {len(filtered_docs)}/{len(documents)} ì„ íƒë¨")
        
        # ìƒìœ„ ê¸°ì‚¬ë“¤ì˜ ì œëª© ë¡œê¹… (ë””ë²„ê¹…ìš©)
        if filtered_docs:
            top_titles = [doc.get("title", "ì œëª©ì—†ìŒ")[:50] for doc in filtered_docs[:3]]
            self.logger.info(f"ìƒìœ„ 3ê°œ ê¸°ì‚¬: {top_titles}")
        else:
            self.logger.warning(f"í‚¤ì›Œë“œ {exact_keywords_str}ê°€ ëª¨ë‘ í¬í•¨ëœ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
        return filtered_docs
    
    def _filter_relevant_documents_relaxed(self, documents: List[Dict[str, Any]], keywords: List[str], question: str) -> List[Dict[str, Any]]:
        """ê´€ë ¨ì„± ê¸°ë°˜ ë¬¸ì„œ í•„í„°ë§ - ì™„í™”ëœ ê¸°ì¤€"""
        
        if not documents or not keywords:
            return documents
        
        filtered_docs = []
        
        for doc in documents:
            title = doc.get("title", "").lower()
            content = doc.get("content", "").lower()
            full_text = f"{title} {content}"
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
            keyword_matches = 0
            for keyword in keywords[:5]:  # ìµœëŒ€ 5ê°œ í‚¤ì›Œë“œ ì²´í¬
                if keyword.lower() in full_text:
                    keyword_matches += 1
            
            # ì™„í™”ëœ ê¸°ì¤€: 50% ì´ìƒ í‚¤ì›Œë“œê°€ ë§¤ì¹­ë˜ë©´ ê´€ë ¨ ê¸°ì‚¬ë¡œ ì¸ì •
            threshold = max(1, len(keywords[:5]) * 0.5)  # ìµœì†Œ 1ê°œ, ìµœëŒ€ 50% ê¸°ì¤€
            
            if keyword_matches >= threshold:
                # ì œëª©ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš° ìš°ì„  ìˆœìœ„ ë¶€ì—¬
                title_score = sum(1 for kw in keywords[:5] if kw.lower() in title)
                # BigKinds ì ìˆ˜ë„ ê³ ë ¤
                bigkinds_score = doc.get("_score", 0) / 100.0 if doc.get("_score") else 0
                
                doc["_relevance_score"] = title_score * 2 + keyword_matches + bigkinds_score
                filtered_docs.append(doc)
        
        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬ (ë†’ì€ ì ìˆ˜ ìš°ì„ )
        filtered_docs.sort(key=lambda x: x.get("_relevance_score", 0), reverse=True)
        
        self.logger.info(f"ì™„í™”ëœ í•„í„°ë§: {keywords[:5]} ì¤‘ {threshold}ê°œ ì´ìƒ í¬í•¨, {len(filtered_docs)}/{len(documents)} ì„ íƒë¨")
        
        return filtered_docs
    
    def _get_keyword_synonyms(self, keyword: str) -> List[str]:
        """í‚¤ì›Œë“œì˜ ë™ì˜ì–´ ë° ìœ ì‚¬ì–´ë¥¼ ë°˜í™˜ (ì •í™•ë„ ìš°ì„  ê°œì„ )"""
        
        keyword_lower = keyword.lower()
        synonyms = []
        
        # í™•ì¥í•˜ì§€ ë§ì•„ì•¼ í•  ì •í™•í•œ í‚¤ì›Œë“œë“¤
        exact_keywords = {
            "ì‚¼ì„±ì „ì", "lgì „ì", "skí•˜ì´ë‹‰ìŠ¤", "í˜„ëŒ€ì°¨", "í˜„ëŒ€ìë™ì°¨",
            "ë„¤ì´ë²„", "ì¹´ì¹´ì˜¤", "í¬ìŠ¤ì½”", "ì…€íŠ¸ë¦¬ì˜¨", "ë°”ì´ì˜¤ë‹ˆì•„",
            "hbm", "gpu", "cpu", "ai", "chatgpt", "llm", "nft", 
            "ë©”íƒ€ë²„ìŠ¤", "iot", "5g", "6g", "esg"
        }
        
        # ì •í™•í•œ í‚¤ì›Œë“œì¸ ê²½ìš° í™•ì¥í•˜ì§€ ì•ŠìŒ
        if keyword_lower in exact_keywords:
            self.logger.info(f"ì •í™•í•œ í‚¤ì›Œë“œ '{keyword}' - í™•ì¥í•˜ì§€ ì•ŠìŒ")
            return []
        
        # ê¸°ì—…ëª… ë™ì˜ì–´ ì‚¬ì „ (í™•ì¥ ì œí•œ)
        company_synonyms = {
            # "ì‚¼ì„±ì „ì": ["ì‚¼ì„±"],  # ì œê±°: ì‚¼ì„±ì „ìëŠ” í™•ì¥í•˜ì§€ ì•ŠìŒ
            "í˜„ëŒ€": ["í˜„ëŒ€ì°¨", "í˜„ëŒ€ìë™ì°¨"],  # í˜„ëŒ€ë§Œ í™•ì¥ í—ˆìš©
            "lg": ["LGì „ì", "LGê·¸ë£¹"],
            "sk": ["SKí…”ë ˆì½¤", "SKì´ë…¸ë² ì´ì…˜"],  # SKí•˜ì´ë‹‰ìŠ¤ëŠ” ì œì™¸
            # "ë„¤ì´ë²„": ["NAVER"],  # ì œê±°: ì •í™•í•œ ë§¤ì¹­ ìš°ì„ 
            # "ì¹´ì¹´ì˜¤": ["KAKAO"],  # ì œê±°: ì •í™•í•œ ë§¤ì¹­ ìš°ì„ 
        }
        
        # ê¸°ìˆ /ì‚°ì—… ë™ì˜ì–´ ì‚¬ì „ (ë§¤ìš° ì œí•œì )
        tech_synonyms = {
            "ì¸ê³µì§€ëŠ¥": ["AI"],  # AIë³´ë‹¤ëŠ” ì¸ê³µì§€ëŠ¥ì´ ë” ì¼ë°˜ì 
            "ë°˜ë„ì²´": ["ì¹©", "ë©”ëª¨ë¦¬"],
            "ì „ê¸°ì°¨": ["EV", "ì „ë™ì°¨"],
            "ë°°í„°ë¦¬": ["ì „ì§€"],
            "ì›ì „": ["ì›ìë ¥"],
        }
        
        # ê²½ì œ/ê¸ˆìœµ ë™ì˜ì–´ ì‚¬ì „
        finance_synonyms = {
            "ì£¼ì‹": ["ì¦ì‹œ", "ì£¼ê°€"],
            "ê¸ˆë¦¬": ["ê¸°ì¤€ê¸ˆë¦¬"],
            "ë¶€ë™ì‚°": ["ì•„íŒŒíŠ¸", "ì£¼íƒ"],
        }
        
        # ëª¨ë“  ì‚¬ì „ì„ í•©ì³ì„œ ê²€ìƒ‰ (ì œí•œì )
        all_synonyms = {**company_synonyms, **tech_synonyms, **finance_synonyms}
        
        # ì •í™•í•œ ë§¤ì¹­ë§Œ í—ˆìš©
        if keyword_lower in all_synonyms:
            synonyms.extend(all_synonyms[keyword_lower])
        
        # ì¤‘ë³µ ì œê±°í•˜ê³  ì›ë³¸ í‚¤ì›Œë“œ ì œì™¸
        synonyms = [s for s in set(synonyms) if s.lower() != keyword_lower]
        
        # ë™ì˜ì–´ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë¡œê·¸ ì¶œë ¥
        if synonyms:
            self.logger.info(f"í‚¤ì›Œë“œ '{keyword}' í™•ì¥: {synonyms}")
        
        return synonyms[:2]  # ìµœëŒ€ 2ê°œì˜ ë™ì˜ì–´ë§Œ ë°˜í™˜ (ê¸°ì¡´ 3ê°œì—ì„œ ì¶•ì†Œ)